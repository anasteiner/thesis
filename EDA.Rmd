
## Time changes for all demographic variables

```{r}
# Ensure the data is sorted correctly by Nationality and Year
final_long <- final_long %>%
  arrange(Nationality, Year, MSOA11CD)

# Remove rows with any NA values
final_long_clean <- final_long %>%
  drop_na() 

ggplot(final_long_clean, aes(x = Year, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +  # Bars side by side
  facet_wrap(~ Nationality) +
  theme_minimal() +
  labs(title = "Dodged Bar Chart of Metrics Over Time",
       x = "Year",
       y = "Value",
       fill = "Metric") +
  theme(legend.position = "bottom")
```

#### Changes in space
```{r}
finalest_census_dt <- st_as_sf(finalest_census_dt)

# Function to create a choropleth map
create_choropleth <- function(nationality, metric, year) {
  ggplot(data = finalest_census_dt %>% filter(Nationality == nationality, Year == year), 
         aes_string(fill = metric)) + 
    geom_sf(color = NA) + 
    scale_fill_viridis_c(option = "plasma") + 
    theme_minimal() +
    labs(title = paste(nationality, "-", metric, "in", year, "by MSOA"),
         fill = metric)
}

# List of nationalities, metrics, and years
nationalities <- c("Bangladesh", "Caribbean", "Poland", "Romania", "SEAfrica", "SouthAmerica")
metrics <- c("Proportion", "Dissimilarity_Index", "cumulative_dissimilarity", "H", "Isolation_Index")
years <- c(2001, 2005, 2011, 2015, 2021)

# Create a directory to save the maps if it doesn't exist
if (!dir.exists("maps")) dir.create("maps")

# Generate maps and save them as files
for (nat in nationalities) {
  for (metric in metrics) {
    for (year in years) {
      map <- create_choropleth(nat, metric, year)
      ggsave(filename = paste0("maps/", nat, "_", metric, "_", year, "_choropleth.png"), plot = map, width = 8, height = 6)
    }
  }
}
```

```{r}
# Calculate summary statistics by Year
summary_by_year <- finalest_census_dt %>%
  group_by(Year) %>%
  summarise(across(where(is.numeric), list(mean = mean, sd = sd, min = min, max = max), na.rm = TRUE))

# Calculate summary statistics by MSOA11CD
summary_by_msoa <- finalest_census_dt %>%
  group_by(MSOA11CD, geometry) %>%
  summarise(across(where(is.numeric), list(mean = mean, sd = sd, min = min, max = max), na.rm = TRUE))

# Display the summary statistics
print(summary_by_year)
print(summary_by_msoa)
```
```{r}
# Write the cleaned data frame to a CSV file
write.csv(summary_by_year, "census_year_data.csv", row.names = FALSE)
# Write the cleaned data frame to a CSV file
write.csv(summary_by_msoa, "census_msoa_data.csv", row.names = FALSE)
```

```{r}
install.packages("spdep")
library(spdep)
```

```{r}
# Ensure the data is in sf format
finalest_census_dt <- st_as_sf(finalest_census_dt)

# Check and set the coordinate reference system (CRS) if needed
finalest_census_dt <- st_transform(finalest_census_dt, crs = 27700)  
```

```{r}
# Remove the H and Isolation_Index columns
finalest_census_dt_cleaned <- finalest_census_dt %>%
  select(-H, -Isolation_Index)
```

## Local Moran I's
```{r}
library(tidyr)

# Reshape the data to wide format
finalest_census_dt_wide <- finalest_census_dt_cleaned %>%
  pivot_wider(names_from = Nationality, 
              values_from = c(Proportion, Dissimilarity_Index, cumulative_dissimilarity))

# Remove NA columns columns
wide_cleaned <- finalest_census_dt_wide %>%
  select(-Proportion_NA, -Dissimilarity_Index_NA, -cumulative_dissimilarity_NA)

```

```{r}
# Check and fix geometries if necessary
wide_cleaned <- st_make_valid(wide_cleaned)
```

```{r}
# Create the neighbors list
neighbors <- poly2nb(wide_cleaned)

# Identify polygons with no neighbors
no_neighbors <- which(sapply(neighbors, length) == 0)

# Print the MSOA11CDs of polygons with no neighbors
print(wide_cleaned$MSOA11CD[no_neighbors])
```
```{r}
valid_geometries <- st_is_valid(wide_cleaned)
print(sum(!valid_geometries))
```

```{r}
library(spdep)
library(sf)
library(ggplot2)
library(dplyr)

# List of variables to analyze
variables <- c(
  "Proportion_Bangladesh", "Proportion_Caribbean", "Proportion_Poland",
  "Proportion_Romania", "Proportion_SEAfrica", "Proportion_SouthAmerica",
  "Dissimilarity_Index_Bangladesh", "Dissimilarity_Index_Caribbean", 
  "Dissimilarity_Index_Poland", "Dissimilarity_Index_Romania", 
  "Dissimilarity_Index_SEAfrica", "Dissimilarity_Index_SouthAmerica",
  "cumulative_dissimilarity_Bangladesh", "cumulative_dissimilarity_Caribbean", 
  "cumulative_dissimilarity_Poland", "cumulative_dissimilarity_Romania", 
  "cumulative_dissimilarity_SEAfrica", "cumulative_dissimilarity_SouthAmerica"
)

# Unique years in the dataset
years <- unique(wide_cleaned$Year)

# Loop through each year and calculate Moran's I for each variable
for (year in years) {
  
  # Filter data for the specific year
  data_year <- wide_cleaned %>% filter(Year == year)
  
  # Convert to sf object if needed
  data_year_sf <- st_as_sf(data_year, crs = 27700)
  
  # Create the neighbors list and spatial weights matrix
  nb <- poly2nb(data_year_sf)
  listw <- nb2listw(nb, zero.policy = TRUE)
  
  for (var in variables) {
    
    # Calculate Local Moran's I
    local_moran <- localmoran(data_year_sf[[var]], listw)
    
    # Add results to the spatial dataframe
    data_year_sf[[paste0(var, "_local_moran_I")]] <- local_moran[,1]  # Moran's I statistic
    data_year_sf[[paste0(var, "_local_moran_p")]] <- local_moran[,5]  # p-value
    
    # Identify clusters
    data_year_sf[[paste0(var, "_cluster")]] <- NA
    data_year_sf[[paste0(var, "_cluster")]][data_year_sf[[paste0(var, "_local_moran_I")]] > 0 & data_year_sf[[paste0(var, "_local_moran_p")]] < 0.05] <- "High-High"
    data_year_sf[[paste0(var, "_cluster")]][data_year_sf[[paste0(var, "_local_moran_I")]] < 0 & data_year_sf[[paste0(var, "_local_moran_p")]] < 0.05] <- "Low-Low"
    data_year_sf[[paste0(var, "_cluster")]][data_year_sf[[paste0(var, "_local_moran_I")]] > 0 & data_year_sf[[paste0(var, "_local_moran_p")]] >= 0.05] <- "Not Significant"
    data_year_sf[[paste0(var, "_cluster")]][data_year_sf[[paste0(var, "_local_moran_I")]] < 0 & data_year_sf[[paste0(var, "_local_moran_p")]] >= 0.05] <- "Not Significant"
    
    # Plot the Local Moran's I values
    p <- ggplot(data_year_sf) +
      geom_sf(aes_string(fill = paste0(var, "_local_moran_I"))) +
      scale_fill_viridis_c(option = "C") +
      labs(title = paste("Local Moran's I for", var, "in", year),
           fill = "Local Moran's I") +
      theme_minimal()
    
    # Save the Moran's I map
    ggsave(paste0(var, "_Local_Morans_I_Map_", year, ".png"), plot = p, width = 7, height = 7)
    
    # Plot the cluster map
    p_cluster <- ggplot(data_year_sf) +
      geom_sf(aes_string(fill = paste0(var, "_cluster"))) +
      scale_fill_manual(values = c("High-High" = "red", "Low-Low" = "blue", "Not Significant" = "grey")) +
      labs(title = paste("Local Moran's I Cluster Map for", var, "in", year),
           fill = "Cluster Type") +
      theme_minimal()
    
    # Save the cluster map
    ggsave(paste0(var, "_Local_Morans_I_Cluster_Map_", year, ".png"), plot = p_cluster, width = 7, height = 7)
    
  }
}

```

## Hotspot analysis
```{r}
library(spdep)
library(sf)
library(ggplot2)

# Ensure 'wide_cleaned' is an sf object
wide_cleaned_sf <- st_as_sf(wide_cleaned)

# Define a function for Hotspot Analysis
calculate_hotspots <- function(data, variable) {
  # Create spatial weights matrix based on contiguity (using queen's case)
  nb <- poly2nb(data)
  listw <- nb2listw(nb, style = "W", zero.policy = TRUE)
  
  # Calculate local G statistics and convert to numeric vector
  gstat <- as.numeric(localG(data[[variable]], listw))
  
  # Add local G statistics to the spatial dataframe
  data[[paste0(variable, "_hotspot")]] <- gstat
  
  return(data)
}

# Apply Hotspot Analysis for each variable and year
for (year in c(2001, 2005, 2011, 2015, 2021)) {
  for (var in c(  "Proportion_Bangladesh", "Proportion_Caribbean", "Proportion_Poland",
                  "Proportion_Romania", "Proportion_SEAfrica", "Proportion_SouthAmerica",
                  "Dissimilarity_Index_Bangladesh", "Dissimilarity_Index_Caribbean", 
                  "Dissimilarity_Index_Poland", "Dissimilarity_Index_Romania", 
                  "Dissimilarity_Index_SEAfrica", "Dissimilarity_Index_SouthAmerica",
                  "cumulative_dissimilarity_Bangladesh", "cumulative_dissimilarity_Caribbean", 
                  "cumulative_dissimilarity_Poland", "cumulative_dissimilarity_Romania", 
                  "cumulative_dissimilarity_SEAfrica", "cumulative_dissimilarity_SouthAmerica")) {
    
    # Filter data for the specific year
    data_year <- wide_cleaned_sf %>% filter(Year == year)
    
    # Perform hotspot analysis
    data_year_hotspot <- calculate_hotspots(data_year, var)
    
    # Plot the Hotspot Map
    p_hotspot <- ggplot(data_year_hotspot) +
      geom_sf(aes_string(fill = paste0(var, "_hotspot"))) +
      scale_fill_viridis_c(option = "C") +
      labs(title = paste("Hotspot Analysis for", var, "in", year),
           fill = "Local G* Value") +
      theme_minimal()
    
    # Save the Hotspot Map
    ggsave(paste0(var, "_Hotspot_Map_", year, ".png"), plot = p_hotspot, width = 7, height = 7)
  }
}

```

## EDA of housing components
```{r}
str(final_housing$DisaggregatedRent)

# Identify non-numeric entries
non_numeric <- final_housing[!is.na(as.numeric(final_housing$DisaggregatedRent)) == FALSE, ]
print(non_numeric)

# Calculate summary statistics by Year, ignoring NA values
housing_by_year <- final_housing %>%
  group_by(Year) %>%
  summarise(across(
    where(is.numeric),
    list(
      mean = ~mean(.x, na.rm = TRUE),
      sd = ~sd(.x, na.rm = TRUE),
      min = ~min(.x, na.rm = TRUE),
      max = ~max(.x, na.rm = TRUE)
    )
  ))

# Calculate summary statistics by MSOA11CD, ignoring NA values
housing_by_msoa <- final_housing %>%
  group_by(MSOA11CD, geometry) %>%
  summarise(across(
    where(is.numeric),
    list(
      mean = ~mean(.x, na.rm = TRUE),
      sd = ~sd(.x, na.rm = TRUE),
      min = ~min(.x, na.rm = TRUE),
      max = ~max(.x, na.rm = TRUE)
    )
  ))

# Save the dataset as a CSV file
write.csv(housing_by_year, "C:/Users/USUARIO/Documents/UCL/Dissertation/v3 longitudinal diaspora study/Code/housing by year.csv", row.names = FALSE)
write.csv(housing_by_msoa, "C:/Users/USUARIO/Documents/UCL/Dissertation/v3 longitudinal diaspora study/Code/housing by msoa.csv", row.names = FALSE)
```

# Annual changes for each variable
```{r}
# List of variables to plot
variables_to_plot <- c(
    "DisaggregatedBudget_mean",
    "DisaggregatedRent_mean",
    "Occupation_mean",
    "Houseprice_mean",
    "average_owned_mean",
    "average_rented_mean",
    "average_social_mean"
)

# Loop through each variable to create individual plots
for (var in variables_to_plot) {
  p <- ggplot(housing_by_year, aes_string(x = "Year", y = var)) +
    geom_line(color = "blue") +
    geom_point(color = "red") +
    labs(title = paste("Year vs", var),
         x = "Year",
         y = var) +
    theme_minimal()
  
  # Save each plot as a PNG file
  ggsave(filename = paste0(var, "housing_trend.png"), plot = p)}
```

# MSOA changes for each variable
```{r}
# List of variables to create thematic maps
variables_to_plot <- c(
    "DisaggregatedBudget_mean",
    "DisaggregatedRent_mean",
    "Occupation_mean",
    "Houseprice_mean",
    "average_owned_mean",
    "average_rented_mean",
    "average_social_mean"
)

housing_by_msoa <- st_as_sf(housing_by_msoa, crs = 27700)

# Loop through each variable to create and save thematic maps
for (var in variables_to_plot) {
  p <- ggplot(housing_by_msoa) +
    geom_sf(aes_string(fill = var)) +
    scale_fill_viridis_c() +  # Using Viridis color scale
    labs(title = paste("Thematic Map of", var),
         fill = var) +
    theme_minimal()
  
  # Save each map as a PNG file
  ggsave(filename = paste0(var, "housing_map.png"), plot = p)
  
  # Print the plot to the R console
  print(p)
}
```

## EDA neighborhood data

```{r}
# Changes in crime over time

# Calculate the change in crime rate from the first to the last year for each MSOA
crime_change <- neighborhood_cleanest %>%
  group_by(MSOA11CD) %>%
  summarize(change = last(MSOA_crime) - first(MSOA_crime))

# Classify the change
crime_change <- crime_change %>%
  mutate(change_category = case_when(
    change > 0 ~ "Increased",
    change < 0 ~ "Decreased",
    TRUE ~ "No Change"
  ))

# Merge with the spatial data
neighborhood_cleanest <- left_join(neighborhood_cleanest, crime_change, by = "MSOA11CD")

# Convert the 'geometry' column to an sf object, assuming WKT format
neighborhood_cleanester <- st_as_sf(neighborhood_cleanest, crs = 27700)

# Plot the map
tm_shape(neighborhood_cleanester) +
  tm_polygons("change_category", palette = c("red", "green", "grey"),
              title = "Crime Rate Change") +
  tm_layout(title = "Change in Crime Rates by MSOA")
```

```{r}
# Green space

# Use the most recent year for the green space percentage
green_space <- neighborhood_cleanester %>%
  filter(Year == max(Year))

# Plot the map
tm_shape(green_space) +
  tm_polygons("MSOA_percent_green", palette = "Greens", title = "Percent Green Space") +
  tm_layout(title = "Green Space Percentage by MSOA")
```

```{r}
# PTAL

# Use the most recent year for PTAL
ptal_data <- neighborhood_cleanester %>%
  filter(Year == max(Year))

# Plot the map
tm_shape(ptal_data) +
  tm_polygons("PTAL", palette = "Blues", title = "PTAL Level") +
  tm_layout(title = "Public Transport Accessibility Level by MSOA")

```

```{r}
# Changes in crime

# Filter data for specific years
selected_years <- neighborhood_cleanester %>% filter(Year %in% c(2001, 2005, 2011, 2015, 2021))

# Function to create map for a specific year
create_map <- function(data, year) {
  ggplot(data) +
    geom_sf(aes(fill = MSOA_crime)) +
    scale_fill_viridis_c() +
    theme_minimal() +
    ggtitle(paste("Crime Data for the Year", year)) +
    theme(legend.position = "bottom")
}

# Loop through each year and plot
for(year in c(2001, 2005, 2011, 2015, 2021)) {
  map_data <- selected_years %>% filter(Year == year)
  print(create_map(map_data, year))
}

```


```{r}
# Changes in religion

# Function to plot religion distribution for a specific year
plot_religion_map <- function(year) {
  religion_data <- neighborhood_cleanester %>%
    filter(Year == year)
  
  tm_shape(religion_data) +
    tm_polygons("Religion", palette = "Set3", title = "Religion") +
    tm_layout(title = paste("Religion Distribution in", year))
}

# Plot maps for the selected years
plot_religion_map(2001)
plot_religion_map(2005)
plot_religion_map(2011)
plot_religion_map(2015)
plot_religion_map(2021)

```

```{r}
# Correlation

library(dplyr)
library(corrplot)

# Select the variables of interest for the correlation matrix
df_selected <- neighborhood_cleanest %>%
  dplyr::select(MSOA_crime, MSOA_percent_green, PTAL, Religion)

# Convert 'Religion' to a numeric variable if it's categorical
df_selected$Religion <- as.numeric(as.factor(df_selected$Religion))

# Ensure all columns are numeric (if needed)
df_selected <- df_selected %>%
  mutate_if(is.character, as.numeric) %>%
  mutate_if(is.factor, as.numeric)

df_no_geometry <- df_selected %>%
  st_drop_geometry()

# Calculate the correlation matrix
correlation_matrix <- cor(df_no_geometry, use = "complete.obs")

# Print the correlation matrix
print(correlation_matrix)

library(corrplot)
# Visualize the correlation matrix
corrplot(correlation_matrix, method = "circle", type = "upper", order = "hclust",
         tl.col = "black", tl.srt = 45, addCoef.col = "black", number.cex = 0.7)
```

