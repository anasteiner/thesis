This document shows the code for the cleaning and joining of the demographic "census" data, and creation of its dataset. It later includes the joining and cleaning of the complete data set (census data + housing + neighborhood data), and the most important part, the statistical and advanced analysis. 

## Load necessary libraries

```{r}
install.packages(c("readxl", "dplyr", "tidyr", "stats", "readr", "zoo", 
                   "ggplot2", "sf", "tmap", "sp", "spdep", "MASS", 
                    "tidyverse", "stringr", "ineq", "FactoMineR", 
                    "factoextra", "reshape2"))

# Load libraries
library(readxl)        
library(dplyr)        
library(tidyr)         
library(stats)         
library(readr)         
library(zoo)           
library(ggplot2) 
library(sp)            
library(MASS)         
library(tidyverse)    
library(stringr)       
library(ineq)          
library(factoextra)  
library(reshape2)      
```

## Load census data

```{r}
setwd("/Users/anamariasteinercorrea/Desktop/v3 longitudinal diaspora study/Data")

# Demographic data
census_2001 <- read_csv("/Users/anamariasteinercorrea/Desktop/v3 longitudinal diaspora study/Data/census 2001.csv")
census_2011 <- read_csv("/Users/anamariasteinercorrea/Desktop/v3 longitudinal diaspora study/Data/census 2011 vf.csv")
census_2021 <- read_csv("/Users/anamariasteinercorrea/Desktop/v3 longitudinal diaspora study/Data/census 2021.csv")

install.packages("sf", type = "binary")
library(sf)

# Define file paths for the shapefiles
msoa_path <- "/Users/anamariasteinercorrea/Desktop/v3 longitudinal diaspora study/Data/MSOA_2011_London_gen_MHW.shp"
lsoa_path <- "/Users/anamariasteinercorrea/Desktop/v3 longitudinal diaspora study/Data/LSOA_2011_London_gen_MHW.shp"
borough_path <- "/Users/anamariasteinercorrea/Desktop/v3 longitudinal diaspora study/Data/London_Borough_Excluding_MHW.shp"

# Read the shapefiles using st_read
msoa <- st_read(msoa_path, quiet = FALSE)
lsoa <- st_read(lsoa_path, quiet = FALSE)
borough <- st_read(borough_path, quiet = FALSE)
```

## Clean and join all data

### Spatial data

```{r}
# Eliminate columns
msoa <- dplyr::select(msoa, -USUALRES, -HHOLDRES, -COMESTRES, -POPDEN, -HHOLDS, -AVHHOLDSZ, -RGN11NM, -RGN11CD)

# Eliminate columns
lsoa <- dplyr::select(lsoa, -MSOA11CD, -MSOA11NM, -LAD11CD, -LAD11NM, -RGN11CD, -RGN11NM, -USUALRES, -HHOLDRES, -COMESTRES, -POPDEN, -HHOLDS, -AVHHOLDSZ)

# Eliminate columns
borough <- dplyr::select(borough, -HECTARES, -NONLD_AREA, -ONS_INNER, -SUB_2009, -SUB_2006)
```

### Demographic data

##### Census 2001

```{r}
# Load necessary packages
library(dplyr)

# Clean columns
names(census_2001)[names(census_2001) == "geography"] <- "MSOA11NM"
names(census_2001)[names(census_2001) == "geography code"] <- "MSOA11CD"
clean_2001 <- na.omit(census_2001)  
clean_2001$year <- 2001  

# Exclude unwanted columns
column_indices <- c(1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 46, 48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85)
cleaner_2001 <- clean_2001[, column_indices]

# Clean the nationality names and convert to numeric codes
cleaner_2001 <- cleaner_2001 %>%
  rename_with(~ gsub("Country of Birth: ", "", .x))

# Identify nationality columns 
non_nationality_columns <- c("MSOA11CD", "MSOA11NM", "date", "All categories: Country of birth; measures: Value")
nationality_columns <- setdiff(names(cleaner_2001), non_nationality_columns)

# Ensure all nationality columns are numeric
cleaner_2001[nationality_columns] <- lapply(cleaner_2001[nationality_columns], as.numeric)

# Add majority nationality value column
cleaner_2001 <- cleaner_2001 %>%
  rowwise() %>%
  mutate(
    Majority_Nationality_Value = max(c_across(all_of(nationality_columns)))
  ) %>%
  ungroup()

# Select only the total population and specified nationalities
specific_nationalities <- c("Romania; measures: Value", "Poland; measures: Value", 
                            "Jamaica; measures: Value", "Other Caribbean and West Indies; measures: Value", 
                            "Somalia; measures: Value", "Kenya; measures: Value", 
                            "South Africa; measures: Value", "Zimbabwe; measures: Value", 
                            "Other  South and Eastern Africa; measures: Value", "Bangladesh; measures: Value", 
                            "South America; measures: Value")

# Column for total population
total_population_column <- "All categories: Country of birth; measures: Value"

# Eliminate columns
final_2001 <- cleaner_2001 %>%
  dplyr::select(MSOA11NM, MSOA11CD, date, Majority_Nationality_Value, total_population_column, all_of(specific_nationalities))

# Clean the column names
final_2001 <- final_2001 %>%
  rename_with(~ gsub("; measures: Value", "", .x))

# Create new columns for aggregated nationalities
library(dplyr)

final_2001 <- final_2001 %>%
  mutate(
    Caribbean = `Jamaica` + `Other Caribbean and West Indies`,
    SEAfrica = `Somalia` + `Kenya` + `South Africa` + `Zimbabwe` + `Other  South and Eastern Africa`,
    SouthAmerica = `South America`
  ) %>%
  dplyr::select(-`Jamaica`, -`Other Caribbean and West Indies`, -`Somalia`, -`Kenya`, -`South Africa`, -`Zimbabwe`, 
                -`Other  South and Eastern Africa`, -`South America`)

final_2001 <- final_2001 %>%
  rename(
    Total_Population_2001 = `All categories: Country of birth`,
    Majority_2001 = `Majority_Nationality_Value`,
    Caribbean_2001 = Caribbean,
    SEAfrica_2001 = SEAfrica,
    Poland_2001 = Poland,
    Romania_2001 = Romania,
    Bangladesh_2001 = Bangladesh,
    SouthAmerica_2001 = SouthAmerica
  )
```

##### Census 2011

```{r}
# Cleaning
names(census_2011)[names(census_2011) == "geography"] <- "MSOA11NM"
names(census_2011)[names(census_2011) == "geography code"] <- "MSOA11CD"
clean_2011 <- na.omit(census_2011)  
clean_2011$year <- 2011  

# Eliminate columns
column_indices <- c(1, 2, 3, 5, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 34, 35, 37, 39, 40, 41, 43, 44, 45, 46, 47, 48, 51, 52, 54, 55, 56, 58, 59, 60, 61, 62, 64, 65, 69, 70, 71, 72, 74, 75, 77, 79, 80, 81, 82)
cleaner_2011 <- clean_2011[, column_indices]

# Rename columns
specific_columns_corrected <- c(
  "Country of Birth: All categories: Country of birth; measures: Value",
  "Country of Birth: Europe: United Kingdom: England; measures: Value",
  "Country of Birth: Europe: Other Europe: EU countries: Accession countries April 2001 to March 2011: Romania; measures: Value",
  "Country of Birth: Europe: Other Europe: EU countries: Accession countries April 2001 to March 2011: Poland; measures: Value",
"Country of Birth: Africa: South and Eastern Africa: Kenya; measures: Value",                                                  "Country of Birth: Africa: South and Eastern Africa: Somalia; measures: Value",
"Country of Birth: Africa: South and Eastern Africa: South Africa; measures: Value",                                           "Country of Birth: Africa: South and Eastern Africa: Zimbabwe; measures: Value",                                               "Country of Birth: Africa: South and Eastern Africa: Other South and Eastern Africa; measures: Value", 
  "Country of Birth: Middle East and Asia: Southern Asia: Bangladesh; measures: Value",
  "Country of Birth: The Americas and the Caribbean: South America; measures: Value",
"Country of Birth: The Americas and the Caribbean: The Caribbean: Jamaica; measures: Value",                                                        "Country of Birth: The Americas and the Caribbean: The Caribbean: Other Caribbean; measures: Value" 
)

specific_nationalities <- c("total population", "Majority", "Romania", "Poland", "Kenya", "Somalia", "South Africa", "Zimbabwe", "Other South and Eastern Africa", "Bangladesh", "South America", "Jamaica", "Other Caribbean")

# Create final dataset
final_2011 <- cleaner_2011 %>%
  dplyr::select(MSOA11CD, MSOA11NM, date, all_of(specific_columns_corrected)) %>%
  dplyr::rename_with(~ specific_nationalities, .cols = all_of(specific_columns_corrected))

final_2011 <- final_2011 %>%
  dplyr::mutate(
    Caribbean = `Jamaica` + `Other Caribbean`,
    SEAfrica = `Somalia` + `Kenya` + `South Africa` + `Zimbabwe` + `Other South and Eastern Africa`
  ) %>%
  dplyr::select(-`Jamaica`, -`Other Caribbean`, -`Somalia`, -`Kenya`, -`South Africa`, -`Zimbabwe`, -`Other South and Eastern Africa`)

final_2011 <- final_2011 %>%
  rename(
    Total_Population_2011 = `total population`,
    Majority_2011 = `Majority`,
    Caribbean_2011 = Caribbean,
    SEAfrica_2011 = SEAfrica,
    Poland_2011 = Poland,
    Romania_2011 = Romania,
    Bangladesh_2011 = Bangladesh,
    SouthAmerica_2011 = `South America`
  )
```

##### Census 2021

```{r}
# Initial cleaning
names(census_2021)[names(census_2021) == "Middle layer Super Output Areas"] <- "MSOA11NM"
names(census_2021)[names(census_2021) == "Middle layer Super Output Areas Code"] <- "MSOA11CD"
clean_2021 <- na.omit(census_2021)

# Pivot wider
pivoted_2021 <- clean_2021 %>%
  pivot_wider(
    id_cols = c(MSOA11CD, MSOA11NM),
    names_from = `Country of birth (60 categories)`,
    values_from = Observation,
    values_fill = list(Observation = 0)
  )

clean_pivoted_2021 <- na.omit(pivoted_2021)
clean_pivoted_2021$date <- 2021
head(clean_pivoted_2021)

# Calculate total population from all nationality columns
total_columns <- grep("Europe|Africa|Middle East and Asia|The Americas and the Caribbean", names(clean_pivoted_2021), value = TRUE)
clean_pivoted_2021$Total_Population <- rowSums(clean_pivoted_2021[, total_columns], na.rm = TRUE)

# Define specific nationalities and their columns 
specific_columns <- c(
  "Europe: United Kingdom: England",
  "Europe: United Kingdom: Northern Ireland",
  "Europe: United Kingdom: Scotland",
  "Europe: United Kingdom: Wales",
  "Europe: Other Europe: EU countries: Countries that joined the EU between April 2001 and March 2011: Romania",
  "Europe: Other Europe: EU countries: Countries that joined the EU between April 2001 and March 2011: Poland",
  "Africa: South and Eastern Africa: Somalia",
  "Africa: South and Eastern Africa: Kenya",
  "Africa: South and Eastern Africa: South Africa",
  "Africa: South and Eastern Africa: Zimbabwe",
  "Africa: South and Eastern Africa: Other South and Eastern Africa",
  "Middle East and Asia: Southern Asia: Bangladesh",
  "The Americas and the Caribbean: South America: All South American countries",
  "The Americas and the Caribbean: The Caribbean: Jamaica",
  "The Americas and the Caribbean: The Caribbean: Other Caribbean"
)

# Prepare the final dataset
final_2021 <- clean_pivoted_2021 %>%
  dplyr::select(MSOA11CD, MSOA11NM, date, Total_Population, all_of(specific_columns)) %>%
  dplyr::rename(
    England_2021 = `Europe: United Kingdom: England`,
    Irland_2021 = `Europe: United Kingdom: Northern Ireland`,
    Scotland_2021 = `Europe: United Kingdom: Scotland`,
    Wales_2021 = `Europe: United Kingdom: Wales`,
    Romania_2021 = `Europe: Other Europe: EU countries: Countries that joined the EU between April 2001 and March 2011: Romania`,
    Poland_2021 = `Europe: Other Europe: EU countries: Countries that joined the EU between April 2001 and March 2011: Poland`,
    Somalia_2021 = `Africa: South and Eastern Africa: Somalia`,
    SEAfrica_2021 = `Africa: South and Eastern Africa: Other South and Eastern Africa`,
    Bangladesh_2021 = `Middle East and Asia: Southern Asia: Bangladesh`,
    SouthAmerica_2021 = `The Americas and the Caribbean: South America: All South American countries`,
    Kenya_2021 = `Africa: South and Eastern Africa: Kenya`,
    Zimbabwe_2021 = `Africa: South and Eastern Africa: Zimbabwe`,
    SAfrica_2021 = `Africa: South and Eastern Africa: South Africa`,
    Jamaica_2021 = `The Americas and the Caribbean: The Caribbean: Jamaica`,
    Caribbean_2021 = `The Americas and the Caribbean: The Caribbean: Other Caribbean`
  )

final_2021 <- final_2021 %>%
  dplyr::mutate(
    Caribbean_2021 = `Jamaica_2021` + `Caribbean_2021`,
    SEAfrica_2021 = `Somalia_2021` + `Kenya_2021` + `SAfrica_2021` + `Zimbabwe_2021` + `SEAfrica_2021`,
    Majority_2021 = `England_2021` + `Irland_2021` + `Scotland_2021` + `Wales_2021`
  ) %>%
  dplyr::select(-`Jamaica_2021`, -`Somalia_2021`, -`Kenya_2021`, -`SAfrica_2021`, -`Zimbabwe_2021`)

final_2021 <- final_2021 %>%
  dplyr::select(-England_2021, -Irland_2021, -Scotland_2021, -Wales_2021)
```

#### Create a complete census dataset

```{r}
# Merge the datasets by MSOA codes and names
combined_census <- final_2001 %>%
  full_join(final_2011, by = c("MSOA11CD", "MSOA11NM")) %>%
  full_join(final_2021, by = c("MSOA11CD", "MSOA11NM"))

# Clean NA values
final_census <- na.omit(combined_census)

# Eliminate columns
final_census <- dplyr::select(final_census, -date, -date.x, -date.y)

final_census <- final_census %>%
  rename(Total_Population_2021 = Total_Population)
```

#### Interpolate demogrpahic data set for non-census years

```{r}

colnames(final_census)
names(final_census)[17] <- "Total_Population_2021"
colnames(final_census)
names(final_census)[17] <- "Caribbean_2011"
names(final_census)[19] <- "Total_Population_2021"

# Transform the data into a long format with separate Nationality and Year columns
data_long <- final_census %>%
  pivot_longer(
    cols = -c(MSOA11CD, MSOA11NM), 
    names_to = c("Nationality", "Year"), 
    names_pattern = "(.*)_(\\d{4})",
    values_to = "Population"
  )

# Convert Year to numeric
data_long$Year <- as.numeric(data_long$Year)

# Interpolate for odd years between 2001 and 2021
library(zoo)
data_long_interpolated <- data_long %>%
  group_by(MSOA11CD, Nationality) %>%
  complete(Year = seq(2001, 2021, by = 1)) %>%
  arrange(Year) %>%
  mutate(Population = na.approx(Population, rule = 2))

# Filter to keep only odd years
data_long_odd_years <- data_long_interpolated %>%
  filter(Year %% 2 == 1)

```

#### Join geometry to final dataset

```{r}
library(tidyr)

# Fill down the MSOA11NM column
data_long_odd_years <- data_long_odd_years %>%
  group_by(MSOA11CD) %>%
  fill(MSOA11NM, .direction = "downup") %>%
  ungroup()

# Join to msoa shp
msoa_census <- left_join(msoa, data_long_odd_years, by = c("MSOA11NM", "MSOA11CD"))
```

#### Interpolate demogrpahic data set for non-census years

```{r}
# Calculate proportions of each nationality
census_final <- msoa_census %>%
  group_by(MSOA11CD, Year) %>%
  mutate(
    Total_Pop = Population[Nationality == "Total_Population"],
    Proportion = ifelse(Nationality == "Total_Population", 1, Population / Total_Pop)
  ) %>%
  ungroup()
```

## Calculation of 3 Segregation analysis

```{r}
# Replace zero proportions with a small value
census_final$Proportion <- ifelse(census_final$Proportion == 0, 0.00001, census_final$Proportion)
```

### Dissimilarity index

#### Dissimiliarity index

```{r}
library(data.table)

# Convert to data.table
census_final_dt <- as.data.table(census_final)

# Calculate the dissimilarity index
dissimilarity_index_dt <- census_final_dt[
  , Majority_Prop := Proportion[Nationality == "Majority"], by = .(Year, MSOA11CD)
][
  Nationality != "Majority", # Exclude Majority group
  .(D_Index = 0.5 * sum(abs(Proportion - Majority_Prop))), by = .(Year, MSOA11CD, Nationality)
]

# Join the D_Index back to the original dataset
census_final_updated_dt <- census_final_dt[
  dissimilarity_index_dt,
  on = .(Year, MSOA11CD, Nationality),
  D_Index := i.D_Index
]
```

#### Lagged diss (-1)

```{r}
# Calculate the lagged Dissimilarity Index and handle 2001 case
census_final_updated_dt <- census_final_updated_dt %>%
  arrange(MSOA11CD, Nationality, Year) %>% 
  group_by(MSOA11CD, Nationality) %>% 
  mutate(
    Lagged_Dissimilarity = lag(D_Index), 
    Lagged_Dissimilarity = ifelse(is.na(Lagged_Dissimilarity) & Year == 2001, D_Index, Lagged_Dissimilarity) 
  ) %>%
  ungroup() 

# Print the updated dataset
print(census_final_updated_dt)
```

### Theil's H index

```{r}
# Ensure the dataset is a data.table
census_final_updated_dt <- as.data.table(census_final_updated_dt)

# Calculate Theil's H
theils_h_results <- census_final_updated_dt[
  Nationality != "Majority", 
  .(Theils_H = sum(ifelse(Proportion > 0, Proportion * log(1 / Proportion), 0), na.rm = TRUE)), # Compute Theil's H
  by = .(MSOA11CD, Year) 
]

# Merge Theil's H results back into the original dataset
census_final_updated_dt <- census_final_updated_dt[
  theils_h_results,
  on = .(MSOA11CD, Year),
  Theils_H := i.Theils_H # Add Theil's H as a new column
]
```

### Isolation index

```{r}
library(data.table)

# Normalize proportions within the subset (excluding Majority)
census_final_updated_dt[
  Nationality != "Majority",
  Proportion_Subset := Proportion / sum(Proportion, na.rm = TRUE),
  by = .(MSOA11CD, Year)
]

# Calculate the Isolation Index for the subset
isolation_results_dt <- census_final_updated_dt[
  Nationality != "Majority",
  .(Isolation_Index = sum(Proportion_Subset^2, na.rm = TRUE)), 
  by = .(MSOA11CD, Year)
]

# Merge Isolation Index back into the original dataset
census_final_updated_dt[
  isolation_results_dt,
  on = .(MSOA11CD, Year),
  Isolation_Index := i.Isolation_Index
]

# View the updated dataset
print(census_final_updated_dt)
```

```{r}
# Final cleaning

# Filter out rows where Nationality is "Total_Population" or "Majority"
census_final_updated_dt <- census_final_updated_dt[!(Nationality %in% c("Total_Population", "Majority"))]

# Remove the columns "Total_Pop" and "Population"
census_final_updated_dt <- census_final_updated_dt[, !c("Total_Pop", "Population"), with = FALSE]
```

```{r}
# Remove rows with any NA values
census_data_clean <- na.omit(census_final_updated_dt)
```

```{r}
# Loop through each nationality and calculate statistics
for (nationality in unique(census_data_clean$Nationality)) {
  # Filter data for the current nationality
  nat_data <- census_data_clean %>%
    filter(Nationality == nationality)
  
  # Calculate statistics
  min_value <- min(nat_data$D_Index, na.rm = TRUE)
  max_value <- max(nat_data$D_Index, na.rm = TRUE)
  median_value <- median(nat_data$D_Index, na.rm = TRUE)
  mean_value <- mean(nat_data$D_Index, na.rm = TRUE)
  
  # Print results for the current nationality
  cat("Nationality:", nationality, "\n")
  cat("Min:", min_value, "\n")
  cat("Max:", max_value, "\n")
  cat("Median:", median_value, "\n")
  cat("Mean:", mean_value, "\n\n")
}
```

### Visualice histograms and boxplots of distributions

```{r}
# Directory for histogram and boxplot outputs
dir.create("distribution_plots", showWarnings = FALSE)

# Histograms for all indices
for (var in c("Proportion", "D_Index", "Isolation_Index", "Theils_H")) {
  hist_plot <- ggplot(census_data_clean, aes_string(x = var)) +
    geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
    facet_wrap(~ Nationality, scales = "free") +
    labs(title = paste("Histogram of", var, "by Nationality"),
         x = var, y = "Count") +
    theme_minimal()
  
  # Save histogram
  ggsave(filename = paste0("distribution_plots/Histogram_", var, ".png"),
         plot = hist_plot, width = 10, height = 6)
}

# Boxplots for all indices
for (var in c("Proportion", "D_Index", "Isolation_Index", "Theils_H")) {
  box_plot <- ggplot(census_data_clean, aes(x = Nationality, y = !!sym(var))) +
    geom_boxplot(fill = "lightblue", color = "black", alpha = 0.7) +
    labs(title = paste("Boxplot of", var, "by Nationality"),
         x = "Nationality", y = var) +
    theme_minimal()
  
  # Save boxplot
  ggsave(filename = paste0("distribution_plots/Boxplot_", var, ".png"),
         plot = box_plot, width = 10, height = 6)
}
```

### Visualice temporal line graphs of all indices

```{r}
library(ggplot2)
library(dplyr)

# Ensure Year is numeric
census_data_clean$Year <- as.numeric(census_data_clean$Year)

# Aggregate data by Year and Nationality
aggregated_data <- census_data_clean %>%
  group_by(Year, Nationality) %>%
  summarize(
    Proportion = mean(Proportion, na.rm = TRUE),
    D_Index = mean(D_Index, na.rm = TRUE),
    Lagged_Dissimilarity = mean(Lagged_Dissimilarity, na.rm = TRUE),
    Theils_H = mean(Theils_H, na.rm = TRUE),
    Isolation_Index = mean(Isolation_Index, na.rm = TRUE),
    .groups = "drop"
  )

# Create directory for nationality-wise plots
dir.create("line_graphs_nationality", showWarnings = FALSE)

# Loop through each nationality and create line plots
nationalities <- unique(aggregated_data$Nationality)

for (nationality in nationalities) {
  # Filter data for the current nationality
  nat_data <- aggregated_data %>%
    filter(Nationality == nationality)

  # Generate the line plot
  plot <- ggplot(nat_data) +
    geom_line(aes(x = Year, y = Proportion, color = "Proportion"), size = 1.2) +
    geom_line(aes(x = Year, y = D_Index, color = "Dissimilarity Index"), size = 1.2) +
    geom_line(aes(x = Year, y = Theils_H, color = "Theil's H"), size = 1.2) +
    geom_line(aes(x = Year, y = Isolation_Index, color = "Isolation Index"), size = 1.2) +
    labs(
      title = paste("Temporal Trends for", nationality),
      x = "Year", y = "Value", color = "Index"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      legend.position = "top"
    )

  # Save the plot
  ggsave(
    filename = paste0("line_graphs_nationality/Trends_", nationality, ".png"),
    plot = plot,
    width = 10,
    height = 6
  )
}
```

### Visualice and compare temporal line graph between all nationalities

```{r}
# Create line graph for Dissimilarity Index comparing all nationalities
dissimilarity_plot <- ggplot(aggregated_data, aes(x = Year, y = D_Index, color = Nationality, group = Nationality)) +
  geom_line(size = 1.2) +
  geom_point(size = 2) +
  scale_y_continuous(
    breaks = scales::pretty_breaks(n = 6), 
    labels = c(0.00, 0.15, 0.3, 0.45, 0.6, 0.75) 
  ) +
  labs(
    title = "Dissimilarity Index Over Time (All Nationalities)",
    x = "Year",
    y = "Dissimilarity Index", # Add a note for clarity
    color = "Nationality" # Legend title
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title.y = element_text(face = "bold", size = 12), 
    legend.title = element_text(face = "bold"), 
    legend.position = "right" 
  )

# Save the plot
dir.create("line_graphs_comparison", showWarnings = FALSE)
ggsave(
  filename = "line_graphs_comparison/Dissimilarity_Index_All_Nationalities_Custom_Labels.png",
  plot = dissimilarity_plot, width = 10, height = 6
)
```

### Calculate spatial autocorrelation of diss index between nationalities

```{r}
library(sf)
library(dplyr)
library(ggplot2)
library(Matrix)
library(FNN)

# Ensure consistent CRS for geometries
census_data_clean <- census_data_clean %>%
  st_as_sf() %>% 
  st_transform(crs = 27700) 

# Create unique geometries with MSOA codes
unique_geometries <- census_data_clean %>%
  group_by(MSOA11CD) %>%
  slice(1) %>% 
  ungroup() %>% 
  dplyr::select(MSOA11CD, geometry) %>% 
  st_as_sf() 

# Ensure CRS is consistent for unique_geometries
unique_geometries <- st_transform(unique_geometries, crs = 27700)

# Get unique nationalities
nationalities <- unique(census_data_clean$Nationality)

# Loop through each nationality
for (nationality in nationalities) {
  # Filter data for the current nationality
  nat_data <- census_data_clean %>%
    filter(Nationality == nationality) %>%
    dplyr::select(MSOA11CD, Year, D_Index, geometry) %>%
    distinct()

  # Use st_join for spatial join (ensuring proper handling of sf objects)
  nat_data <- st_join(unique_geometries, nat_data, left = TRUE)

  # Ensure nat_data is still an sf object
  nat_data <- st_as_sf(nat_data)

  # Prepare dissimilarity index values and handle missing data
  x <- nat_data$D_Index
  x[is.na(x)] <- mean(x, na.rm = TRUE)  
  x_mean <- mean(x, na.rm = TRUE)

  # Recompute centroids and weights matrix for this subset
  coords <- st_coordinates(st_centroid(st_geometry(nat_data)))
  knn <- get.knn(coords, k = 10)

  # Create a sparse weights matrix for this subset
  weights <- sparseMatrix(
    i = rep(1:nrow(knn$nn.index), each = 10),
    j = as.vector(knn$nn.index),
    x = 1,
    dims = c(nrow(coords), nrow(coords))
  )

  # Row-standardize the weights matrix
  row_sums <- rowSums(weights)
  row_standardized_weights <- weights / row_sums

  # Compute Local Moran's I manually
  local_moran <- numeric(length(x))
  for (i in seq_along(x)) {
    neighbors <- which(row_standardized_weights[i, ] > 0)
    lagged_value <- sum(row_standardized_weights[i, neighbors] * x[neighbors], na.rm = TRUE)
    local_moran[i] <- (x[i] - x_mean) * (lagged_value - x_mean)
  }

  # Normalize Local Moran's I
  local_moran <- local_moran / var(x, na.rm = TRUE)

  # Add Moran's I results to nat_data
  nat_data <- nat_data %>%
    mutate(Local_I = local_moran,
           Quadrant = case_when(
             (D_Index > x_mean & Local_I > 0) ~ "High-High",
             (D_Index < x_mean & Local_I > 0) ~ "Low-Low",
             (D_Index > x_mean & Local_I < 0) ~ "High-Low",
             (D_Index < x_mean & Local_I < 0) ~ "Low-High",
             TRUE ~ "Non-significant"
           ))

  # Create LISA Map
  lisa_map <- ggplot(data = nat_data) +
    geom_sf(aes(fill = Quadrant), color = NA) +
    scale_fill_manual(values = c("High-High" = "red", "Low-Low" = "blue",
                                 "High-Low" = "orange", "Low-High" = "green",
                                 "Non-significant" = "grey")) +
    labs(title = paste("LISA Map for", nationality),
         fill = "Quadrant") +
    theme_minimal()

  # Save LISA Map
  if (!dir.exists("moran_outputs")) dir.create("moran_outputs")
  ggsave(filename = paste0("moran_outputs/LISA_Map_", nationality, ".png"),
         plot = lisa_map, width = 10, height = 8)

  # Print progress
  print(paste("Completed Moran's I and LISA Map for:", nationality))
}
```

### Visualice maps of diss index between nationalities and through time

```{r}
library(ggplot2)
library(sf)

# Create directory for spatial maps
dir.create("spatial_maps", showWarnings = FALSE)

# Loop through each nationality to create maps
nationalities <- unique(census_data_clean$Nationality)

for (nationality in nationalities) {
  # Filter data for the current nationality
  nat_data <- census_data_clean %>%
    filter(Nationality == nationality)
  
  # Plot map for the current nationality
  diss_map <- ggplot(nat_data) +
    geom_sf(aes(fill = D_Index, geometry = geometry), color = NA) +
    scale_fill_viridis_c(option = "C", name = "Dissimilarity Index") +
    labs(
      title = paste("Dissimilarity Index Across London -", nationality),
      caption = "Source: Census Data"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      legend.position = "right"
    )
  
  # Save the plot
  ggsave(
    filename = paste0("spatial_maps/Dissimilarity_Index_", nationality, ".png"),
    plot = diss_map,
    width = 12,
    height = 8
  )
}
```

## Maps with Borough information
```{r}
library(ggplot2)
library(sf)
library(dplyr)
library(grid)

# Add numeric IDs for boroughs
borough <- borough %>%
  mutate(Borough_ID = row_number())

# Create directory for spatial maps
dir.create("spatial_maps", showWarnings = FALSE)

# Loop through each nationality to create maps
nationalities <- unique(census_data_clean$Nationality)

for (nationality in nationalities) {
  # Filter data for the current nationality
  nat_data <- census_data_clean %>%
    filter(Nationality == nationality)
  
  # Base map
  diss_map <- ggplot() +
    geom_sf(data = nat_data, aes(fill = D_Index, geometry = geometry), color = NA) +
    geom_sf(data = borough, aes(geometry = geometry), fill = NA, color = "black", linetype = "solid", size = 1) +  # Increased line thickness
    geom_sf_text(data = borough, aes(label = Borough_ID, geometry = geometry), size = 4, color = "black", fontface = "bold") +  # Black and bold numbers
    scale_fill_viridis_c(option = "C", name = "Dissimilarity Index") +
    labs(
      title = paste("Dissimilarity Index Across London -", nationality),
      caption = "Source: Census Data",
      subtitle = "Numbers correspond to borough names in the legend"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      legend.position = "right"
    )
  
  # Create legend as a separate grob
  borough_legend <- ggplot(data = borough) +
    geom_text(aes(x = 0, y = -Borough_ID, label = paste(Borough_ID, NAME, sep = ": ")), hjust = 0, size = 4) +
    theme_void() +
    labs(title = "Borough Legend") +
    theme(
      plot.title = element_text(size = 12, face = "bold", hjust = 0.5)
    )
  
  # Combine map and legend using gridExtra
  final_plot <- cowplot::plot_grid(
    diss_map,
    borough_legend,
    ncol = 2,
    rel_widths = c(3, 1)  # Adjust relative widths
  )
  
  # Save the combined plot
  ggsave(
    filename = paste0("spatial_maps/Dissimilarity_Index_", nationality, ".png"),
    plot = final_plot,
    width = 16,
    height = 10
  )
}

print("Maps with embedded legends saved successfully.")
```

## Anual borough maps

```{r}
library(ggplot2)
library(sf)
library(dplyr)

# Create directory for spatial maps by year
dir.create("spatial_maps_years", showWarnings = FALSE)

# Filter census_data_clean for the specific years
filtered_years <- census_data_clean %>% filter(Year %in% c(2001, 2011, 2021))

# Loop through each nationality and year
nationalities <- unique(filtered_years$Nationality)

for (nationality in nationalities) {
  for (year in c(2001, 2011, 2021)) {
    # Filter data for the current nationality and year
    nat_year_data <- filtered_years %>%
      filter(Nationality == nationality & Year == year)

    # Plot map for the current nationality and year
    map <- ggplot() +
      geom_sf(data = nat_year_data, aes(fill = D_Index, geometry = geometry), color = NA) +
      geom_sf(data = borough, fill = NA, color = "black", size = 0.6) +
      geom_sf_text(data = borough, aes(label = NAME), size = 3, color = "black", fontface = "bold") +
      scale_fill_viridis_c(option = "C", name = "Dissimilarity Index") +
      labs(
        title = paste("Dissimilarity Index Across London -", nationality, "-", year),
        caption = "Source: Census Data"
      ) +
      theme_minimal() +
      theme(
        plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
        legend.position = "right"
      )

    # Save the plot
    ggsave(
      filename = paste0("spatial_maps_years/Dissimilarity_Index_", nationality, "_", year, ".png"),
      plot = map,
      width = 12,
      height = 8
    )
  }
}

print("Maps by nationality and year have been created and saved!")
```

### Identify MSOAs with Most Changes in Dissimilarity Index

```{r}
# Calculate the top 3 boroughs (LAD) with most and least segregation
top_boroughs <- census_data_clean %>%
  group_by(LAD11NM, Nationality) %>%
  summarise(Average_Segregation = mean(D_Index, na.rm = TRUE)) %>%
  arrange(desc(Average_Segregation)) %>%
  dplyr::slice(1:3) # Top 3 most segregated

least_boroughs <- census_data_clean %>%
  group_by(LAD11NM, Nationality) %>%
  summarise(Average_Segregation = mean(D_Index, na.rm = TRUE)) %>%
  arrange(Average_Segregation) %>%
  slice(1:3) # Top 3 least segregated
```

```{r}
# Calculate the top 3 boroughs with the greatest increase/decrease in segregation
segregation_change <- census_data_clean %>%
  group_by(LAD11NM, Nationality) %>%
  summarise(Change = last(D_Index) - first(D_Index)) %>%
  ungroup() %>% 
  arrange(desc(Change)) %>%
  dplyr::slice(1:3)

segregation_decrease <- census_data_clean %>%
  group_by(LAD11NM, Nationality) %>%
  summarise(Change = last(D_Index) - first(D_Index)) %>%
  ungroup() %>% 
  arrange(Change) %>%
  dplyr::slice(1:3) 
```

# Load all packages

```{r}
library(tidyverse)
library(sf)
library(sp)
library(spgwr)
library(ggplot2)
library(gridExtra)
library(cowplot)
library(xts)
library(zoo)
```

```{r}
# Read the datasets
setwd("/Users/anamariasteinercorrea/thesis")
housing_data <- read_csv("/Users/anamariasteinercorrea/Documents/thesis/housing_data.csv")
neighborhood_data <- read_csv("/Users/anamariasteinercorrea/Documents/thesis/neighborhood_data.csv")
msoa_geometry <- st_read("/Users/anamariasteinercorrea/Desktop/v3 longitudinal diaspora study/Data/MSOA_2011_London_gen_MHW.shp")
```

## Clean and join all the datasets for final analysis

```{r}
# Perform the join of housing + neighborhood
hn_data <- housing_data %>%
  left_join(neighborhood_data, by = c("MSOA11NM", "MSOA11CD", "Year"))

aggregated_hn <- hn_data %>%
  group_by(MSOA11CD, MSOA11NM, Year) %>%  # Group by MSOA and Year
  summarize(across(where(is.numeric), mean, na.rm = TRUE), .groups = "drop")  # Average numeric columns
```

```{r}
segregation_analysis <- aggregated_hn %>%
  mutate(Year = as.character(Year)) %>% 
  left_join(census_data_clean %>% mutate(Year = as.character(Year)),  
            by = c("MSOA11NM", "MSOA11CD", "Year"))  

# Check the result
print(segregation_analysis)
```

```{r}
# Join with spatial data
segregation_final <- segregation_analysis %>%
  left_join(msoa_geometry %>% distinct(MSOA11CD, MSOA11NM, .keep_all = TRUE), 
            by = c("MSOA11CD", "MSOA11NM"))

# Convert to sf object
segregation_sf <- st_as_sf(segregation_final)

# Clean the dataset
final_segregation <- segregation_sf %>%
  dplyr::select(
    MSOA11CD,
    MSOA11NM,
    Year,
    Rent,
    Socialhousing = budget_proportion,
    Owned,
    Houseprice,
    Overoccupation = Occupation,
    geometry = geometry.y,
    Crime = MSOA_crime,
    Greenery = MSOA_percent_green,
    PTAL,
    Nationality,
    Proportion,
    D_Index,
    Lagged_Dissimilarity,
  ) 

filtered_segregation <- final_segregation %>%
  filter(MSOA11NM != "London")
```

## Correlations and Linear regressions

```{r}
# Load required libraries
library(tidyverse)

# Scaling all variables
scaled_segregation <- filtered_segregation %>%
  mutate(
    # Housing variables
    scaled_Rent = -1 * (Rent / 1000),                   # Rent scale
    scaled_Socialhousing = Socialhousing / 100,        # Scale Socialhousing
    scaled_Houseprice = -1 * (Houseprice / 100000),    # Houseprice scale
    scaled_Overoccupation = Overoccupation / 1000,     # Scale Overoccupation
    scaled_Owned = Owned,                              # Keep Owned unchanged
    # Neighborhood variables
    scaled_Crime = Crime / 10,                         # Scale Crime
    scaled_Greenery = Greenery,                        # Keep Greenery unchanged
    scaled_PTAL = PTAL  
  )
```

### Correlation matrix for each nationality

```{r}
library(ggcorrplot)

# Directory for correlation matrices
dir.create("correlation_matrices", showWarnings = FALSE)

# Loop through each nationality
for (nationality in unique(scaled_segregation$Nationality)) {
  nat_data <- scaled_segregation %>%
    filter(Nationality == nationality) %>%
    st_drop_geometry()  
  
  correlation_data <- nat_data %>%
    dplyr::select(
      D_Index, scaled_Rent, scaled_Socialhousing, scaled_Owned, scaled_Houseprice,
      scaled_Overoccupation, scaled_Crime, scaled_Greenery, scaled_PTAL, Lagged_Dissimilarity
    )
  
  correlation_data <- correlation_data %>%
    na.omit()
  
  correlation_data <- correlation_data %>%
    mutate(across(everything(), as.numeric))
  
  if (nrow(correlation_data) > 0 && ncol(correlation_data) > 1) {
    corr_matrix <- cor(correlation_data, use = "complete.obs")
    
    corr_plot <- ggcorrplot(corr_matrix, method = "circle",
                            title = paste("Correlation Matrix for", nationality)) +
      theme_minimal()
    
    ggsave(filename = paste0("correlation_matrices/correlation_matrix_", nationality, ".png"),
           plot = corr_plot, width = 10, height = 6)
  } else {
    message(paste("Not enough valid data for correlation for nationality:", nationality))
  }
}
```

```{r}
# Load required libraries
library(dplyr)

predictors <- scaled_segregation %>%
  st_drop_geometry() %>%  
  dplyr::select(scaled_Rent, scaled_Socialhousing, scaled_Owned, scaled_Houseprice,
      scaled_Overoccupation, scaled_Crime, scaled_Greenery, scaled_PTAL,
      Lagged_Dissimilarity) %>% 
  na.omit()  

# Function to calculate VIF manually
calculate_vif <- function(data) {
  vif_values <- sapply(names(data), function(var) {
    formula <- as.formula(paste(var, "~ ."))
    r_squared <- summary(lm(formula, data = data))$r.squared  # Get R²
    vif <- 1 / (1 - r_squared)  # Calculate VIF
    return(vif)
  })
  return(vif_values)
}

# Calculate VIF for predictors
vif_results <- calculate_vif(predictors)

# Print VIF results
print(vif_results)
```

### Scatterplot for individual correlations

```{r}
library(ggplot2)
library(dplyr)
library(MASS)  

# Ensure directories for outputs exist
if (!dir.exists("scatterplots_fixed")) dir.create("scatterplots_fixed")
if (!dir.exists("regression_outputs")) dir.create("regression_outputs")

# Specify variables for scatterplots and regression
x_vars <- c("scaled_Rent", "scaled_Socialhousing", "scaled_Owned", "scaled_Houseprice",
            "scaled_Overoccupation", "scaled_Crime", "scaled_Greenery", "scaled_PTAL", "Lagged_Dissimilarity")

# Loop through each nationality
for (nationality in unique(scaled_segregation$Nationality)) {
  nat_data <- scaled_segregation %>%
    filter(Nationality == nationality) %>%
    mutate(Dissimilarity_Index = as.numeric(D_Index))  # Ensure numeric
  
  # Create PDF for diagnostics
  pdf(file = paste0("regression_outputs/Diagnostics_", nationality, ".pdf"))
  
  for (x_var in x_vars) {
    if (x_var %in% colnames(nat_data)) {  
      clean_data <- nat_data %>%
        filter(
          !is.na(Dissimilarity_Index), !is.na(!!sym(x_var)),
          Dissimilarity_Index < quantile(D_Index, 0.99, na.rm = TRUE),
          !!sym(x_var) < quantile(!!sym(x_var), 0.99, na.rm = TRUE)
        )
      
      if (nrow(clean_data) < 5) {
        warning(paste("Not enough data for regression for nationality:", nationality, "and predictor:", x_var))
        next
      }
      
      # Fit simple regression model (lm by default)
      regression_formula <- paste("D_Index ~", x_var)
      model <- lm(as.formula(regression_formula), data = clean_data)
      
      # Check diagnostics: Heteroscedasticity
      diagnostics_problem <- function(model) {
        res <- residuals(model)
        fitted_vals <- fitted(model)
        abs(cor(res^2, fitted_vals)) > 0.3  # Example threshold
      }
      
      # Switch to robust regression if diagnostics indicate issues
      if (diagnostics_problem(model)) {
        message(paste("Switching to robust regression for:", nationality, "-", x_var))
        model <- rlm(as.formula(regression_formula), data = clean_data)
      }
      
      par(mfrow = c(2, 2))  # 2x2 layout for diagnostics
      plot(model, main = paste("Diagnostics for", nationality, "-", x_var))
      
      scatter_plot <- ggplot(clean_data, aes(y = D_Index, x = !!sym(x_var))) +
        geom_point(alpha = 0.5, color = "blue") +
        geom_smooth(method = "lm", se = FALSE, color = "red") +
        labs(
          title = paste("Scatterplot for", nationality, "-", x_var),
          x = x_var,
          y = "Dissimilarity Index"
        ) +
        theme_minimal() +
        scale_y_continuous(expand = c(0.2, 0))
      
      ggsave(
        filename = paste0("scatterplots_fixed/scatterplot_", nationality, "_", x_var, ".png"),
        plot = scatter_plot,
        width = 10,
        height = 6
      )
    }
  }
  
  dev.off()  
}
```

```{r}
ggplot(scaled_segregation, aes(x = scaled_Rent, y = D_Index, color = factor(cut(scaled_Houseprice, breaks = 3)))) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(color = "House Price Level")

```


### CCFs for individual correlations

```{r}
library(dplyr)

# Ensure a directory for CCF outputs
if (!dir.exists("ccfs")) dir.create("ccfs")

# List of variables to analyze against Dissimilarity_Index
x_vars <- c("scaled_Rent", "scaled_Socialhousing", "scaled_Houseprice", "scaled_Overoccupation", "scaled_Owned")

# Ensure all relevant variables are numeric and drop geometry in the process
aggregated_ccf <- scaled_segregation %>%
  st_drop_geometry() %>%  # Drop geometry to handle numeric-only processing
  mutate(
    scaled_Rent = as.numeric(scaled_Rent),
    scaled_Socialhousing = as.numeric(scaled_Socialhousing),
    scaled_Houseprice = as.numeric(scaled_Houseprice),
    scaled_Overoccupation = as.numeric(scaled_Overoccupation),
    scaled_Owned = as.numeric(scaled_Owned)
  ) %>%
  group_by(Year, Nationality) %>%  # Group by Year and Nationality
  summarize(
    mean_diss = mean(D_Index, na.rm = TRUE),  # Aggregate dissimilarity index
    across(
      c(scaled_Rent, scaled_Socialhousing, scaled_Houseprice, scaled_Overoccupation, scaled_Owned),
      ~ mean(.x, na.rm = TRUE)
    ),
    .groups = "drop"
  )

# Check the structure to confirm numeric conversion
str(aggregated_ccf)

# Check if aggregation was successful
if (nrow(aggregated_data) == 0) {
  stop("No valid data after aggregation. Check input dataset.")
}

# Loop through each nationality
for (nationality in unique(aggregated_data$Nationality)) {
  # Filter data for the current nationality
  nat_data <- aggregated_data %>%
    filter(Nationality == nationality) %>%
    arrange(Year)  # Ensure data is sorted by year

  # Check if sufficient data exists
  if (nrow(nat_data) < 5) {  # Require at least 5 years of data for meaningful CCF
    message(paste("Skipping Nationality:", nationality, "- Not enough data"))
    next
  }

  # Loop through each variable to compute CCF
  for (x_var in x_vars) {
    if (x_var %in% colnames(nat_data)) {  # Ensure variable exists
      # Extract data for CCF
      y_data <- nat_data$mean_diss
      x_data <- nat_data[[x_var]]

      # Check if sufficient and valid data exists
      if (var(y_data) > 0 && var(x_data) > 0) {  # Avoid constant or invalid data
        # Save CCF plot
        png(filename = paste0("ccfs/ccf_London_", nationality, "_", x_var, ".png"), width = 800, height = 600)
        ccf(
          y_data, x_data,
          lag.max = 10, plot = TRUE,
          main = paste("CCF for London - Nationality:", nationality, "-", x_var)
        )
        dev.off()
      } else {
        message(paste("Skipping Nationality:", nationality, "Variable:", x_var, "- Insufficient or constant data"))
      }
    } else {
      message(paste("Variable", x_var, "not found for Nationality:", nationality))
    }
  }
}
```

### Linear regressions

Base regressions (1-5 housing characteristics) Run separate regressions for each housing variable to assess their individual contributions to the segregation index. No. 6 Neighborhood data Add neighborhood-level variables to create a comprehensive model. No. 7 Lagged settlement model Incorporate the lagged settlement variable alongside all housing variables to capture temporal dependencies.

```{r}
# Load necessary libraries
library(stargazer)  # For LaTeX/HTML-style regression tables
library(dplyr)      # For data manipulation

# Define variable groups
housing_variables <- c("scaled_Rent", "scaled_Socialhousing", "scaled_Houseprice", "scaled_Overoccupation", "scaled_Owned")
neighborhood_variables <- c("scaled_Crime", "scaled_Greenery", "scaled_PTAL")
all_variables <- c(housing_variables, "Lagged_Dissimilarity", neighborhood_variables)

# Create directory for outputs
output_dir <- "regression_outputs"
if (!dir.exists(output_dir)) dir.create(output_dir)

# Loop through each nationality
for (nationality in unique(scaled_segregation$Nationality)) {
  # Filter data for the current nationality
  nationality_data <- scaled_segregation %>%
    filter(Nationality == nationality)
  
  if (nrow(nationality_data) == 0) {
    warning(paste("No valid data for nationality:", nationality))
    next
  }
  
  models <- list(
    lm(D_Index ~ scaled_Rent + scaled_Socialhousing + scaled_Houseprice, data = nationality_data),
    lm(D_Index ~ scaled_Rent + scaled_Socialhousing + scaled_Houseprice + scaled_Overoccupation, data = nationality_data),
    lm(D_Index ~ scaled_Rent + scaled_Socialhousing + scaled_Houseprice + scaled_Overoccupation + scaled_Owned, data = nationality_data),
    lm(D_Index ~ scaled_Rent + scaled_Socialhousing + scaled_Houseprice + scaled_Overoccupation + scaled_Owned + Lagged_Dissimilarity, data = nationality_data),
    lm(D_Index ~ scaled_Rent * scaled_Houseprice + scaled_Rent + scaled_Socialhousing + scaled_Houseprice + scaled_Overoccupation + scaled_Owned + Lagged_Dissimilarity +
         scaled_Crime + scaled_Greenery + scaled_PTAL, data = nationality_data))
  
  stargazer(
    models,
    type = "html",
    out = paste0(output_dir, "/RegressionTable_", nationality, ".html"),
    title = paste("Regression Results for", nationality, "(Including Rent × House Price Interaction)"),
    dep.var.labels = "Dissimilarity Index",
    covariate.labels = c(
      "Rent", "Social Housing", "House Price", "Overoccupation", "Owned", "Lagged Dissimilarity",
      "Crime", "Greenery", "PTAL", "Rent × House Price"
    ),
    star.cutoffs = c(0.05, 0.01, 0.001),
    digits = 3,
    coef = lapply(all_models, function(model) { coef(model) * 10 }),  # Scale coefficients
    se = lapply(all_models, function(model) { summary(model)$coefficients[, "Std. Error"] * 10 })  # Scale standard errors
  )
}
```

## GWR

```{r}
library(dplyr)
library(sf)

# Convert dataset to sf object and set CRS 
scaled_segregation_sf <- scaled_segregation %>%
  st_as_sf(wkt = "geometry", crs = 27700)

# Aggregate data for General GWR (20-Year Average)
general_gwr_data <- scaled_segregation_sf %>%
  group_by(MSOA11CD) %>%
  summarize(
    mean_diss = mean(D_Index, na.rm = TRUE),  # Average dissimilarity
    mean_rent = mean(scaled_Rent, na.rm = TRUE),
    mean_socialhousing = mean(scaled_Socialhousing, na.rm = TRUE),
    mean_houseprice = mean(scaled_Houseprice, na.rm = TRUE),
    mean_overoccupation = mean(scaled_Overoccupation, na.rm = TRUE),
    mean_crime = mean(scaled_Crime, na.rm = TRUE),
    mean_ptal = mean(scaled_PTAL, na.rm = TRUE),
    mean_greenery = mean(scaled_Greenery, na.rm = TRUE),
    mean_owned = mean(scaled_Owned, na.rm = TRUE),
    mean_lagged_diss = mean(Lagged_Dissimilarity, na.rm = TRUE),
    geometry = first(geometry)  
  ) %>%
  st_as_sf()  

# Convert geometries to centroids
general_gwr_data_centroids <- general_gwr_data %>%
  st_centroid()

# Remove rows with NaN values in relevant columns
general_gwr_data_clean <- general_gwr_data_centroids %>%
  filter(complete.cases(mean_diss, mean_rent, mean_socialhousing, 
                        mean_houseprice, mean_overoccupation, 
                        mean_owned, mean_lagged_diss, mean_ptal, mean_greenery, mean_crime))

# Check if geometries are valid
library(sf)
general_gwr_data_clean <- general_gwr_data_clean %>%
  mutate(is_valid = st_is_valid(geometry)) %>%
  filter(is_valid)  # Keep only valid geometries

# Transform to a projected CRS 
general_gwr_data_clean <- st_transform(general_gwr_data_clean, crs = 27700)

# Convert to SpatialPointsDataFrame
general_gwr_sp <- as(general_gwr_data_clean, "Spatial")

# Define dependent and independent variables
dependent_var <- "mean_diss"
independent_vars <- c("mean_rent", "mean_socialhousing", "mean_houseprice", 
                      "mean_overoccupation", "mean_owned", "mean_lagged_diss", "mean_crime", "mean_ptal", "mean_greenery")

# Define GWR formula
gwr_formula <- as.formula(
  paste(dependent_var, "~", paste(independent_vars, collapse = " + "))
)

# Select bandwidth automatically
bandwidth <- gwr.sel(gwr_formula, data = general_gwr_sp)

# Run GWR
gwr_results <- gwr(gwr_formula, data = general_gwr_sp, bandwidth = bandwidth, hatmatrix = TRUE)

# Add GWR results back to sf object
general_gwr_data_clean$gwr_fitted <- gwr_results$SDF$pred
for (var in independent_vars) {
  general_gwr_data_clean[[paste0("gwr_coeff_", var)]] <- gwr_results$SDF[[var]]
}
general_gwr_data_clean$local_r2 <- gwr_results$SDF$localR2
```

```{r}
# Summarize coefficients
summary(general_gwr_data_clean[, grep("gwr_coeff_", colnames(general_gwr_data_clean))])
```

```{r}
str(general_gwr_data_clean)
```

```{r}
library(sf)

# Check for invalid geometries
sum(!st_is_valid(general_gwr_data_clean))  # Should return 0
```

```{r}
# Drop the geometry column from general_gwr_data_clean
general_gwr_data_clean_nonspatial <- general_gwr_data_clean %>%
  st_drop_geometry()

# Perform a regular join using MSOA11CD
merged_gwr <- msoa %>%
  left_join(general_gwr_data_clean_nonspatial, by = "MSOA11CD")
```

```{r}
library(ggplot2)

# Create a unique directory for coefficients
if (!dir.exists("plots/coefficients")) dir.create("plots/coefficients", recursive = TRUE)

# List of coefficients to plot
coefficients <- c("gwr_coeff_mean_rent", "gwr_coeff_mean_socialhousing", 
                  "gwr_coeff_mean_houseprice", "gwr_coeff_mean_overoccupation", 
                  "gwr_coeff_mean_owned", "gwr_coeff_mean_lagged_diss", "gwr_coeff_mean_crime", "gwr_coeff_mean_ptal", "gwr_coeff_mean_greenery")

# Loop through each coefficient and create a map
for (coeff in coefficients) {
  plot <- ggplot(merged_gwr) +
    geom_sf(aes_string(fill = coeff), color = NA) +
    scale_fill_gradient2(
      low = "blue", mid = "white", high = "red", midpoint = 0,
      name = paste("GWR Coeff:", gsub("gwr_coeff_", "", coeff))
    ) +
    theme_minimal() +
    labs(
      title = paste("GWR Coefficient for", gsub("gwr_coeff_", "", coeff)),
      caption = "Data aggregated at MSOA level"
    )
  
  # Save the plot using ggsave
  ggsave(
    filename = paste0("plots/coefficients/GWR_Coeff_", gsub("gwr_coeff_", "", coeff), ".png"),
    plot = plot, width = 10, height = 8
  )
}
```

```{r}
# Create a unique directory for local R^2
if (!dir.exists("plots/r2")) dir.create("plots/r2", recursive = TRUE)

# Create the plot for local R^2
r2_plot <- ggplot(merged_gwr) +
  geom_sf(aes(fill = local_r2), color = NA) +
  scale_fill_gradient(
    low = "yellow", high = "green", name = "Local R^2"
  ) +
  theme_minimal() +
  labs(
    title = "GWR Local R^2 Map",
    caption = "Data aggregated at MSOA level"
  )

# Save the Local R^2 plot
ggsave(
  filename = "plots/r2/GWR_Local_R2.png",
  plot = r2_plot, width = 10, height = 8
)
```

```{r}
library(sf)
library(dplyr)
library(Matrix)
library(FNN)

# Ensure consistent CRS for geometries
merged_gwr <- merged_gwr %>%
  st_as_sf() %>%  # Ensure it is an sf object
  st_transform(crs = 27700)  # Example: British National Grid

# Prepare residuals or other variables for Moran's I analysis
merged_gwr$residuals <- merged_gwr$gwr_fitted - merged_gwr$mean_diss  

# Handle missing data in the chosen variable
x <- merged_gwr$residuals
x[is.na(x)] <- mean(x, na.rm = TRUE)  # Replace NA with mean value
x_mean <- mean(x, na.rm = TRUE)

# Recompute centroids and weights matrix for the dataset
coords <- st_coordinates(st_centroid(st_geometry(merged_gwr)))
knn <- get.knn(coords, k = 10)  # k nearest neighbors (k = 10)

# Create a sparse weights matrix
weights <- sparseMatrix(
  i = rep(1:nrow(knn$nn.index), each = 10),
  j = as.vector(knn$nn.index),
  x = 1,
  dims = c(nrow(coords), nrow(coords))
)

# Row-standardize the weights matrix
row_sums <- rowSums(weights)
row_standardized_weights <- weights / row_sums

# Compute Local Moran's I manually
local_moran <- numeric(length(x))
for (i in seq_along(x)) {
  neighbors <- which(row_standardized_weights[i, ] > 0)
  lagged_value <- sum(row_standardized_weights[i, neighbors] * x[neighbors], na.rm = TRUE)
  local_moran[i] <- (x[i] - x_mean) * (lagged_value - x_mean)
}

# Normalize Local Moran's I
local_moran <- local_moran / var(x, na.rm = TRUE)

# Add Local Moran's I results to merged_gwr
merged_gwr <- merged_gwr %>%
  mutate(Local_I = local_moran)

# Save the results to the "plots" directory
if (!dir.exists("plots")) dir.create("plots", recursive = TRUE)
st_write(merged_gwr, "plots/GWR_Local_Moran_f.geojson", driver = "GeoJSON")

# Print completion message
print("Local Moran's I calculation completed. Results saved to plots/GWR_Local_Moran.geojson.")
```

```{r}
library(ggplot2)

# Create a residuals plot
residuals_plot <- ggplot(merged_gwr) +
  geom_sf(aes(fill = residuals), color = NA) +  
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red", midpoint = 0,
    name = "Residuals"
  ) +
  labs(
    title = "Residuals of GWR Model",
    subtitle = "Visualization of Spatial Residuals",
    caption = "Data aggregated at MSOA level"
  ) +
  theme_minimal()

# Save the plot
if (!dir.exists("plots")) dir.create("plots", recursive = TRUE)
ggsave(
  filename = "plots/GWR_Residuals.png",
  plot = residuals_plot,
  width = 10, height = 8
)
```

```{r}
library(dplyr)
library(sf)
library(ggplot2)

scaled_segregation_sf <- scaled_segregation %>%
  st_as_sf(wkt = "geometry", crs = 27700)

# Create directory for nationality results
if (!dir.exists("plots/nationalities")) dir.create("plots/nationalities", recursive = TRUE)

# Get unique nationalities
nationalities <- unique(scaled_segregation_sf$Nationality)

# Loop through each nationality
for (nat in nationalities) {
  # Filter data for the current nationality
  nat_data <- scaled_segregation_sf %>%
    filter(Nationality == nat) %>%
    group_by(MSOA11CD) %>%
    summarize(
      mean_diss = mean(D_Index, na.rm = TRUE),
      mean_rent = mean(scaled_Rent, na.rm = TRUE),
      mean_socialhousing = mean(scaled_Socialhousing, na.rm = TRUE),
      mean_houseprice = mean(scaled_Houseprice, na.rm = TRUE),
      mean_overoccupation = mean(scaled_Overoccupation, na.rm = TRUE),
      mean_owned = mean(scaled_Owned, na.rm = TRUE),
      mean_crime = mean(scaled_Crime, na.rm = TRUE),
      mean_greenery = mean(scaled_Greenery, na.rm = TRUE),
      mean_ptal = mean(scaled_PTAL, na.rm = TRUE),
      mean_lagged_diss = mean(Lagged_Dissimilarity, na.rm = TRUE),
      geometry = first(geometry)
    ) %>%
    st_as_sf()

  # Convert to centroids and remove invalid geometries
  nat_data <- nat_data %>%
    st_centroid() %>%
    filter(complete.cases(mean_diss, mean_rent, mean_socialhousing, mean_houseprice,
                          mean_overoccupation, mean_owned, mean_crime, mean_greenery, 
                          mean_ptal, mean_lagged_diss)) %>%
    mutate(is_valid = st_is_valid(geometry)) %>%
    filter(is_valid)

  # Convert to SpatialPointsDataFrame
  nat_data_sp <- as(nat_data, "Spatial")

  # Define GWR formula
  dependent_var <- "mean_diss"
  independent_vars <- c("mean_rent", "mean_socialhousing", "mean_houseprice", 
                        "mean_overoccupation", "mean_owned", "mean_crime", 
                        "mean_greenery", "mean_ptal", "mean_lagged_diss")
  gwr_formula <- as.formula(paste(dependent_var, "~", paste(independent_vars, collapse = " + ")))

  # Select bandwidth and run GWR
  bandwidth <- gwr.sel(gwr_formula, data = nat_data_sp)
  gwr_results <- gwr(gwr_formula, data = nat_data_sp, bandwidth = bandwidth, hatmatrix = TRUE)

  # Add GWR results back to the sf object
  nat_data$gwr_fitted <- gwr_results$SDF$pred
  for (var in independent_vars) {
    nat_data[[paste0("gwr_coeff_", var)]] <- gwr_results$SDF[[var]]
  }
  nat_data$local_r2 <- gwr_results$SDF$localR2

  # Drop centroids to prepare for MSOA join
  nat_data_nonspatial <- nat_data %>% st_drop_geometry()

  # Join with MSOA polygons
  merged_nationality <- msoa %>%
    left_join(nat_data_nonspatial, by = "MSOA11CD")

  # Plot coefficients
  for (coeff in independent_vars) {
    coeff_plot <- ggplot(merged_nationality) +
      geom_sf(aes_string(fill = paste0("gwr_coeff_", coeff)), color = NA) +
      scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0,
                           name = paste("GWR Coeff:", coeff)) +
      theme_minimal() +
      labs(
        title = paste("GWR Coefficient for", coeff, "(", nat, ")"),
        caption = "Data aggregated at MSOA level"
      )
    # Save plot
    ggsave(
      filename = paste0("plots/nationalities/GWR_Coeff_", coeff, "_", nat, ".png"),
      plot = coeff_plot, width = 10, height = 8
    )
  }
  print(paste("Completed GWR for nationality:", nat))
}
```

```{r}
# Create directory for year results
if (!dir.exists("plots/years")) dir.create("plots/years", recursive = TRUE)

# Specify the years of interest
selected_years <- c(2001, 2005, 2011, 2015, 2021)

# Loop through the selected years
for (yr in selected_years) {
  # Filter data for the current year
  year_data <- scaled_segregation_sf %>%
    filter(Year == yr) %>%
    group_by(MSOA11CD) %>%
    summarize(
      mean_diss = mean(D_Index, na.rm = TRUE),
      mean_rent = mean(scaled_Rent, na.rm = TRUE),
      mean_socialhousing = mean(scaled_Socialhousing, na.rm = TRUE),
      mean_houseprice = mean(scaled_Houseprice, na.rm = TRUE),
      mean_overoccupation = mean(scaled_Overoccupation, na.rm = TRUE),
      mean_owned = mean(scaled_Owned, na.rm = TRUE),
      mean_crime = mean(scaled_Crime, na.rm = TRUE),
      mean_greenery = mean(scaled_Greenery, na.rm = TRUE),
      mean_ptal = mean(scaled_PTAL, na.rm = TRUE),
      mean_lagged_diss = mean(Lagged_Dissimilarity, na.rm = TRUE),
      geometry = first(geometry)
    ) %>%
    st_as_sf()

  # Convert to centroids and remove invalid geometries
  year_data <- year_data %>%
    st_centroid() %>%
    filter(complete.cases(mean_diss, mean_rent, mean_socialhousing, mean_houseprice,
                          mean_overoccupation, mean_owned, mean_crime, mean_greenery, 
                          mean_ptal, mean_lagged_diss)) %>%
    mutate(is_valid = st_is_valid(geometry)) %>%
    filter(is_valid)

  # Convert to SpatialPointsDataFrame
  year_data_sp <- as(year_data, "Spatial")

  # Define GWR formula
  dependent_var <- "mean_diss"
  independent_vars <- c("mean_rent", "mean_socialhousing", "mean_houseprice", 
                        "mean_overoccupation", "mean_owned", "mean_crime", 
                        "mean_greenery", "mean_ptal", "mean_lagged_diss")
  gwr_formula <- as.formula(paste(dependent_var, "~", paste(independent_vars, collapse = " + ")))

  # Select bandwidth and run GWR
  bandwidth <- gwr.sel(gwr_formula, data = year_data_sp)
  gwr_results <- gwr(gwr_formula, data = year_data_sp, bandwidth = bandwidth, hatmatrix = TRUE)

  # Add GWR results back to the sf object
  year_data$gwr_fitted <- gwr_results$SDF$pred
  for (var in independent_vars) {
    year_data[[paste0("gwr_coeff_", var)]] <- gwr_results$SDF[[var]]
  }
  year_data$local_r2 <- gwr_results$SDF$localR2

  # Drop centroids to prepare for MSOA join
  year_data_nonspatial <- year_data %>% st_drop_geometry()

  # Join with MSOA polygons
  merged_year <- msoa %>%
    left_join(year_data_nonspatial, by = "MSOA11CD")

  # Plot coefficients
  for (coeff in independent_vars) {
    coeff_plot <- ggplot(merged_year) +
      geom_sf(aes_string(fill = paste0("gwr_coeff_", coeff)), color = NA) +
      scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0,
                           name = paste("GWR Coeff:", coeff)) +
      theme_minimal() +
      labs(
        title = paste("GWR Coefficient for", coeff, "(", yr, ")"),
        caption = "Data aggregated at MSOA level"
      )
    # Save plot
    ggsave(
      filename = paste0("plots/years/GWR_Coeff_", coeff, "_", yr, ".png"),
      plot = coeff_plot, width = 10, height = 8
    )
  }
  print(paste("Completed GWR for year:", yr))
}
```

## EDA of housing variables

```{r}
library(ggplot2)

# List of housing variables
housing_vars <- c("scaled_Rent", "scaled_Socialhousing", "scaled_Houseprice", 
                  "scaled_Overoccupation", "scaled_Owned", "scaled_Crime", 
                  "scaled_Greenery", "scaled_PTAL")

# Add D_Index quantiles to the dataset
scaled_segregation <- scaled_segregation %>%
  mutate(D_Index_Quantile = ntile(D_Index, 4))

# Create a directory for boxplots
if (!dir.exists("plots/boxplots")) dir.create("plots/boxplots", recursive = TRUE)

# Loop through each variable and generate a boxplot
for (var in housing_vars) {
  plot <- ggplot(scaled_segregation, aes(x = as.factor(D_Index_Quantile), y = .data[[var]])) +
    geom_boxplot(fill = "lightblue") +
    labs(title = paste(var, "Distribution Across Dissimilarity Quantiles"),
         x = "Dissimilarity Index Quantile", y = var) +
    theme_minimal()
  
  # Save the plot
  ggsave(filename = paste0("plots/boxplots/", var, "_Boxplot.png"), plot = plot, width = 10, height = 8)
  
  print(paste("Boxplot for", var, "completed."))
}
```

```{r}
library(ggplot2)
library(dplyr)

# List of housing variables to analyze
housing_vars <- c("Rent", "Socialhousing", "Owned", "Houseprice", "Overoccupation")

# Create a directory for saving trend plots
if (!dir.exists("plots/trends")) dir.create("plots/trends", recursive = TRUE)

# Drop geometry for aggregation
filtered_segregation_no_geom <- filtered_segregation %>%
  st_drop_geometry()

# Ensure the Year column is numeric
filtered_segregation_no_geom <- filtered_segregation_no_geom %>%
  mutate(Year = as.numeric(Year))

# Filter for odd years between 2001 and 2021
filtered_segregation_no_geom <- filtered_segregation_no_geom %>%
  filter(Year %% 2 == 1 & Year >= 2001 & Year <= 2021)

# Loop through each variable and generate a temporal trend plot
for (var in housing_vars) {
  # Aggregate data by year for the current variable
  yearly_data <- filtered_segregation_no_geom %>%
    group_by(Year) %>%
    summarize(
      mean_value = mean(.data[[var]], na.rm = TRUE), # Average across all MSOAs for each year
      .groups = "drop"
    )
  
  # Generate line plot
  plot <- ggplot(yearly_data, aes(x = Year, y = mean_value)) +
    geom_line(color = "blue", size = 1) +
    geom_point(color = "red", size = 2) +
    labs(
      title = paste("Temporal Trend of", var),
      x = "Year",
      y = paste("Average", var, "(across all MSOAs)")
    ) +
    theme_minimal()
  
  # Save the plot
  ggsave(filename = paste0("plots/trends/", var, "_Trend.png"), plot = plot, width = 10, height = 8)
  
  print(paste("Trend for", var, "completed."))
}
```

```{r}
library(ggplot2)
library(sf)
library(dplyr)

# Create the directory for saving maps
output_dir <- "thesis/EDA_housing/maps"
dir.create(output_dir, showWarnings = FALSE, recursive = TRUE)

# List of variables to loop through
housing_vars <- c("Rent", "Socialhousing", "Owned", "Houseprice", "Overoccupation")

# Loop through each housing variable
for (var in housing_vars) {
  
  # Plot full map for 2001–2021
  full_map <- ggplot(filtered_segregation) +
    geom_sf(aes_string(fill = var, geometry = "geometry"), color = NA) +
    geom_sf(data = borough, fill = NA, color = "black", size = 0.8) +
    geom_sf_text(data = borough, aes(label = NAME), size = 3, color = "black") +
    scale_fill_viridis_c(option = "C", name = var) +
    labs(
      title = paste(var, "Across London (2001–2021)"),
      caption = "Source: Housing Data"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      legend.position = "right"
    )
  
  ggsave(
    filename = paste0(output_dir, "/", var, "_Full_Scope_Map.png"),
    plot = full_map,
    width = 12,
    height = 8
  )
  
  # Filter for specific years (2001, 2011, 2021)
  years_to_plot <- c(2001, 2011, 2021)
  for (year in years_to_plot) {
    yearly_data <- filtered_segregation %>% filter(Year == year)
    
    yearly_map <- ggplot(yearly_data) +
      geom_sf(aes_string(fill = var, geometry = "geometry"), color = NA) +
      geom_sf(data = borough, fill = NA, color = "black", size = 0.8) +
      geom_sf_text(data = borough, aes(label = NAME), size = 3, color = "black") +
      scale_fill_viridis_c(option = "C", name = var) +
      labs(
        title = paste(var, "Across London in", year),
        caption = "Source: Housing Data"
      ) +
      theme_minimal() +
      theme(
        plot.title = element_text(hjust = 0.5, face = "bold"),
        legend.position = "right"
      )
    
    ggsave(
      filename = paste0(output_dir, "/", var, "_", year, "_Map.png"),
      plot = yearly_map,
      width = 12,
      height = 8
    )
  }
}
```









