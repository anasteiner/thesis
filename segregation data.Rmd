This document shows the code for the cleaning and joining of the demographic "census" data, and creation of its dataset.

## Load necessary libraries

```{r}
install.packages(c("readxl", "dplyr", "tidyr", "stats", "readr", "zoo", 
                   "ggplot2", "sf", "tmap", "sp", "spdep", "MASS", 
                    "tidyverse", "stringr", "ineq", "FactoMineR", 
                    "factoextra", "reshape2"))

# Load libraries
library(readxl)        
library(dplyr)        
library(tidyr)         
library(stats)         
library(readr)         
library(zoo)           
library(ggplot2) 
library(sp)            
library(MASS)         
library(tidyverse)    
library(stringr)       
library(ineq)          
library(factoextra)  
library(reshape2)      
```

## Load census data

```{r}
setwd("/Users/anamariasteinercorrea/Desktop/v3 longitudinal diaspora study/Data")

# Demographic data
census_2001 <- read_csv("/Users/anamariasteinercorrea/Desktop/v3 longitudinal diaspora study/Data/census 2001.csv")
census_2011 <- read_csv("/Users/anamariasteinercorrea/Desktop/v3 longitudinal diaspora study/Data/census 2011 vf.csv")
census_2021 <- read_csv("/Users/anamariasteinercorrea/Desktop/v3 longitudinal diaspora study/Data/census 2021.csv")

install.packages("sf", type = "binary")
library(sf)

# Define file paths for the shapefiles
msoa_path <- "/Users/anamariasteinercorrea/Desktop/v3 longitudinal diaspora study/Data/MSOA_2011_London_gen_MHW.shp"
lsoa_path <- "/Users/anamariasteinercorrea/Desktop/v3 longitudinal diaspora study/Data/LSOA_2011_London_gen_MHW.shp"
borough_path <- "/Users/anamariasteinercorrea/Desktop/v3 longitudinal diaspora study/Data/London_Borough_Excluding_MHW.shp"

# Read the shapefiles using st_read
msoa <- st_read(msoa_path, quiet = FALSE)
lsoa <- st_read(lsoa_path, quiet = FALSE)
borough <- st_read(borough_path, quiet = FALSE)
```

## Clean and join all data

### Spatial data

```{r}
# Eliminate columns
msoa <- dplyr::select(msoa, -USUALRES, -HHOLDRES, -COMESTRES, -POPDEN, -HHOLDS, -AVHHOLDSZ, -RGN11NM, -RGN11CD)

# Eliminate columns
lsoa <- dplyr::select(lsoa, -MSOA11CD, -MSOA11NM, -LAD11CD, -LAD11NM, -RGN11CD, -RGN11NM, -USUALRES, -HHOLDRES, -COMESTRES, -POPDEN, -HHOLDS, -AVHHOLDSZ)

# Eliminate columns
borough <- dplyr::select(borough, -HECTARES, -NONLD_AREA, -ONS_INNER, -SUB_2009, -SUB_2006)
```

### Demographic data

##### Census 2001

```{r}
# Load necessary packages
library(dplyr)

# Clean columns
names(census_2001)[names(census_2001) == "geography"] <- "MSOA11NM"
names(census_2001)[names(census_2001) == "geography code"] <- "MSOA11CD"
clean_2001 <- na.omit(census_2001)  
clean_2001$year <- 2001  

# Exclude unwanted columns
column_indices <- c(1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 46, 48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85)
cleaner_2001 <- clean_2001[, column_indices]

# Clean the nationality names and convert to numeric codes
cleaner_2001 <- cleaner_2001 %>%
  rename_with(~ gsub("Country of Birth: ", "", .x))

# Identify nationality columns 
non_nationality_columns <- c("MSOA11CD", "MSOA11NM", "date", "All categories: Country of birth; measures: Value")
nationality_columns <- setdiff(names(cleaner_2001), non_nationality_columns)

# Ensure all nationality columns are numeric
cleaner_2001[nationality_columns] <- lapply(cleaner_2001[nationality_columns], as.numeric)

# Add majority nationality value column
cleaner_2001 <- cleaner_2001 %>%
  rowwise() %>%
  mutate(
    Majority_Nationality_Value = max(c_across(all_of(nationality_columns)))
  ) %>%
  ungroup()

# Select only the total population and specified nationalities
specific_nationalities <- c("Romania; measures: Value", "Poland; measures: Value", 
                            "Jamaica; measures: Value", "Other Caribbean and West Indies; measures: Value", 
                            "Somalia; measures: Value", "Kenya; measures: Value", 
                            "South Africa; measures: Value", "Zimbabwe; measures: Value", 
                            "Other  South and Eastern Africa; measures: Value", "Bangladesh; measures: Value", 
                            "South America; measures: Value")

# Column for total population
total_population_column <- "All categories: Country of birth; measures: Value"

# Eliminate columns
final_2001 <- cleaner_2001 %>%
  dplyr::select(MSOA11NM, MSOA11CD, date, Majority_Nationality_Value, total_population_column, all_of(specific_nationalities))

# Clean the column names
final_2001 <- final_2001 %>%
  rename_with(~ gsub("; measures: Value", "", .x))

# Create new columns for aggregated nationalities
library(dplyr)

final_2001 <- final_2001 %>%
  mutate(
    Caribbean = `Jamaica` + `Other Caribbean and West Indies`,
    SEAfrica = `Somalia` + `Kenya` + `South Africa` + `Zimbabwe` + `Other  South and Eastern Africa`,
    SouthAmerica = `South America`
  ) %>%
  dplyr::select(-`Jamaica`, -`Other Caribbean and West Indies`, -`Somalia`, -`Kenya`, -`South Africa`, -`Zimbabwe`, 
                -`Other  South and Eastern Africa`, -`South America`)

final_2001 <- final_2001 %>%
  rename(
    Total_Population_2001 = `All categories: Country of birth`,
    Majority_2001 = `Majority_Nationality_Value`,
    Caribbean_2001 = Caribbean,
    SEAfrica_2001 = SEAfrica,
    Poland_2001 = Poland,
    Romania_2001 = Romania,
    Bangladesh_2001 = Bangladesh,
    SouthAmerica_2001 = SouthAmerica
  )
```

##### Census 2011

```{r}
# Cleaning
names(census_2011)[names(census_2011) == "geography"] <- "MSOA11NM"
names(census_2011)[names(census_2011) == "geography code"] <- "MSOA11CD"
clean_2011 <- na.omit(census_2011)  
clean_2011$year <- 2011  

# Eliminate columns
column_indices <- c(1, 2, 3, 5, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 34, 35, 37, 39, 40, 41, 43, 44, 45, 46, 47, 48, 51, 52, 54, 55, 56, 58, 59, 60, 61, 62, 64, 65, 69, 70, 71, 72, 74, 75, 77, 79, 80, 81, 82)
cleaner_2011 <- clean_2011[, column_indices]

# Rename columns
specific_columns_corrected <- c(
  "Country of Birth: All categories: Country of birth; measures: Value",
  "Country of Birth: Europe: United Kingdom: England; measures: Value",
  "Country of Birth: Europe: Other Europe: EU countries: Accession countries April 2001 to March 2011: Romania; measures: Value",
  "Country of Birth: Europe: Other Europe: EU countries: Accession countries April 2001 to March 2011: Poland; measures: Value",
"Country of Birth: Africa: South and Eastern Africa: Kenya; measures: Value",                                                  "Country of Birth: Africa: South and Eastern Africa: Somalia; measures: Value",
"Country of Birth: Africa: South and Eastern Africa: South Africa; measures: Value",                                           "Country of Birth: Africa: South and Eastern Africa: Zimbabwe; measures: Value",                                               "Country of Birth: Africa: South and Eastern Africa: Other South and Eastern Africa; measures: Value", 
  "Country of Birth: Middle East and Asia: Southern Asia: Bangladesh; measures: Value",
  "Country of Birth: The Americas and the Caribbean: South America; measures: Value",
"Country of Birth: The Americas and the Caribbean: The Caribbean: Jamaica; measures: Value",                                                        "Country of Birth: The Americas and the Caribbean: The Caribbean: Other Caribbean; measures: Value" 
)

specific_nationalities <- c("total population", "Majority", "Romania", "Poland", "Kenya", "Somalia", "South Africa", "Zimbabwe", "Other South and Eastern Africa", "Bangladesh", "South America", "Jamaica", "Other Caribbean")

# Create final dataset
final_2011 <- cleaner_2011 %>%
  dplyr::select(MSOA11CD, MSOA11NM, date, all_of(specific_columns_corrected)) %>%
  dplyr::rename_with(~ specific_nationalities, .cols = all_of(specific_columns_corrected))

final_2011 <- final_2011 %>%
  dplyr::mutate(
    Caribbean = `Jamaica` + `Other Caribbean`,
    SEAfrica = `Somalia` + `Kenya` + `South Africa` + `Zimbabwe` + `Other South and Eastern Africa`
  ) %>%
  dplyr::select(-`Jamaica`, -`Other Caribbean`, -`Somalia`, -`Kenya`, -`South Africa`, -`Zimbabwe`, -`Other South and Eastern Africa`)

final_2011 <- final_2011 %>%
  rename(
    Total_Population_2011 = `total population`,
    Majority_2011 = `Majority`,
    Caribbean_2011 = Caribbean,
    SEAfrica_2011 = SEAfrica,
    Poland_2011 = Poland,
    Romania_2011 = Romania,
    Bangladesh_2011 = Bangladesh,
    SouthAmerica_2011 = `South America`
  )
```

##### Census 2021

```{r}
# Initial cleaning
names(census_2021)[names(census_2021) == "Middle layer Super Output Areas"] <- "MSOA11NM"
names(census_2021)[names(census_2021) == "Middle layer Super Output Areas Code"] <- "MSOA11CD"
clean_2021 <- na.omit(census_2021)

# Pivot wider
pivoted_2021 <- clean_2021 %>%
  pivot_wider(
    id_cols = c(MSOA11CD, MSOA11NM),
    names_from = `Country of birth (60 categories)`,
    values_from = Observation,
    values_fill = list(Observation = 0)
  )

clean_pivoted_2021 <- na.omit(pivoted_2021)
clean_pivoted_2021$date <- 2021
head(clean_pivoted_2021)

# Calculate total population from all nationality columns
total_columns <- grep("Europe|Africa|Middle East and Asia|The Americas and the Caribbean", names(clean_pivoted_2021), value = TRUE)
clean_pivoted_2021$Total_Population <- rowSums(clean_pivoted_2021[, total_columns], na.rm = TRUE)

# Define specific nationalities and their columns 
specific_columns <- c(
  "Europe: United Kingdom: England",
  "Europe: United Kingdom: Northern Ireland",
  "Europe: United Kingdom: Scotland",
  "Europe: United Kingdom: Wales",
  "Europe: Other Europe: EU countries: Countries that joined the EU between April 2001 and March 2011: Romania",
  "Europe: Other Europe: EU countries: Countries that joined the EU between April 2001 and March 2011: Poland",
  "Africa: South and Eastern Africa: Somalia",
  "Africa: South and Eastern Africa: Kenya",
  "Africa: South and Eastern Africa: South Africa",
  "Africa: South and Eastern Africa: Zimbabwe",
  "Africa: South and Eastern Africa: Other South and Eastern Africa",
  "Middle East and Asia: Southern Asia: Bangladesh",
  "The Americas and the Caribbean: South America: All South American countries",
  "The Americas and the Caribbean: The Caribbean: Jamaica",
  "The Americas and the Caribbean: The Caribbean: Other Caribbean"
)

# Prepare the final dataset
final_2021 <- clean_pivoted_2021 %>%
  dplyr::select(MSOA11CD, MSOA11NM, date, Total_Population, all_of(specific_columns)) %>%
  dplyr::rename(
    England_2021 = `Europe: United Kingdom: England`,
    Irland_2021 = `Europe: United Kingdom: Northern Ireland`,
    Scotland_2021 = `Europe: United Kingdom: Scotland`,
    Wales_2021 = `Europe: United Kingdom: Wales`,
    Romania_2021 = `Europe: Other Europe: EU countries: Countries that joined the EU between April 2001 and March 2011: Romania`,
    Poland_2021 = `Europe: Other Europe: EU countries: Countries that joined the EU between April 2001 and March 2011: Poland`,
    Somalia_2021 = `Africa: South and Eastern Africa: Somalia`,
    SEAfrica_2021 = `Africa: South and Eastern Africa: Other South and Eastern Africa`,
    Bangladesh_2021 = `Middle East and Asia: Southern Asia: Bangladesh`,
    SouthAmerica_2021 = `The Americas and the Caribbean: South America: All South American countries`,
    Kenya_2021 = `Africa: South and Eastern Africa: Kenya`,
    Zimbabwe_2021 = `Africa: South and Eastern Africa: Zimbabwe`,
    SAfrica_2021 = `Africa: South and Eastern Africa: South Africa`,
    Jamaica_2021 = `The Americas and the Caribbean: The Caribbean: Jamaica`,
    Caribbean_2021 = `The Americas and the Caribbean: The Caribbean: Other Caribbean`
  )

final_2021 <- final_2021 %>%
  dplyr::mutate(
    Caribbean_2021 = `Jamaica_2021` + `Caribbean_2021`,
    SEAfrica_2021 = `Somalia_2021` + `Kenya_2021` + `SAfrica_2021` + `Zimbabwe_2021` + `SEAfrica_2021`,
    Majority_2021 = `England_2021` + `Irland_2021` + `Scotland_2021` + `Wales_2021`
  ) %>%
  dplyr::select(-`Jamaica_2021`, -`Somalia_2021`, -`Kenya_2021`, -`SAfrica_2021`, -`Zimbabwe_2021`)

final_2021 <- final_2021 %>%
  dplyr::select(-England_2021, -Irland_2021, -Scotland_2021, -Wales_2021)
```

#### Create a complete census dataset

```{r}
# Merge the datasets by MSOA codes and names
combined_census <- final_2001 %>%
  full_join(final_2011, by = c("MSOA11CD", "MSOA11NM")) %>%
  full_join(final_2021, by = c("MSOA11CD", "MSOA11NM"))

# Clean NA values
final_census <- na.omit(combined_census)

# Eliminate columns
final_census <- dplyr::select(final_census, -date, -date.x, -date.y)

final_census <- final_census %>%
  rename(Total_Population_2021 = Total_Population)
```

#### Interpolate demogrpahic data set for non-census years

```{r}

colnames(final_census)
names(final_census)[17] <- "Total_Population_2021"
colnames(final_census)
names(final_census)[17] <- "Caribbean_2011"
names(final_census)[19] <- "Total_Population_2021"

# Transform the data into a long format with separate Nationality and Year columns
data_long <- final_census %>%
  pivot_longer(
    cols = -c(MSOA11CD, MSOA11NM), 
    names_to = c("Nationality", "Year"), 
    names_pattern = "(.*)_(\\d{4})",
    values_to = "Population"
  )

# Convert Year to numeric
data_long$Year <- as.numeric(data_long$Year)

# Interpolate for odd years between 2001 and 2021
library(zoo)
data_long_interpolated <- data_long %>%
  group_by(MSOA11CD, Nationality) %>%
  complete(Year = seq(2001, 2021, by = 1)) %>%
  arrange(Year) %>%
  mutate(Population = na.approx(Population, rule = 2))

# Filter to keep only odd years
data_long_odd_years <- data_long_interpolated %>%
  filter(Year %% 2 == 1)

```

#### Join geometry to final dataset

```{r}
library(tidyr)

# Fill down the MSOA11NM column
data_long_odd_years <- data_long_odd_years %>%
  group_by(MSOA11CD) %>%
  fill(MSOA11NM, .direction = "downup") %>%
  ungroup()

# Join to msoa shp
msoa_census <- left_join(msoa, data_long_odd_years, by = c("MSOA11NM", "MSOA11CD"))
```

#### Interpolate demogrpahic data set for non-census years

```{r}
# Calculate proportions of each nationality
census_final <- msoa_census %>%
  group_by(MSOA11CD, Year) %>%
  mutate(
    Total_Pop = Population[Nationality == "Total_Population"],
    Proportion = ifelse(Nationality == "Total_Population", 1, Population / Total_Pop)
  ) %>%
  ungroup()
```

## Calculation of 3 Segregation analysis

```{r}
# Replace zero proportions with a small value
census_final$Proportion <- ifelse(census_final$Proportion == 0, 0.00001, census_final$Proportion)
```

### Dissimilarity index

#### Dissimiliarity index

```{r}
library(data.table)

# Convert to data.table
census_final_dt <- as.data.table(census_final)

# Step 1: Calculate the dissimilarity index
dissimilarity_index_dt <- census_final_dt[
  , Majority_Prop := Proportion[Nationality == "Majority"], by = .(Year, MSOA11CD)
][
  Nationality != "Majority", # Exclude Majority group
  .(D_Index = 0.5 * sum(abs(Proportion - Majority_Prop))), by = .(Year, MSOA11CD, Nationality)
]

# Step 2: Join the D_Index back to the original dataset
census_final_updated_dt <- census_final_dt[
  dissimilarity_index_dt,
  on = .(Year, MSOA11CD, Nationality),
  D_Index := i.D_Index
]
```

#### Lagged diss (-1)

```{r}
# Calculate the lagged Dissimilarity Index and handle 2001 case
census_final_updated_dt <- census_final_updated_dt %>%
  arrange(MSOA11CD, Nationality, Year) %>% # Ensure data is sorted by MSOA, Nationality, and Year
  group_by(MSOA11CD, Nationality) %>% # Group by MSOA and Nationality
  mutate(
    Lagged_Dissimilarity = lag(D_Index), # Calculate the lagged value
    Lagged_Dissimilarity = ifelse(is.na(Lagged_Dissimilarity) & Year == 2001, D_Index, Lagged_Dissimilarity) # Repeat 2001 value
  ) %>%
  ungroup() # Remove grouping

# Print the updated dataset
print(census_final_updated_dt)
```

### Theil's H index

```{r}
# Ensure the dataset is a data.table
census_final_updated_dt <- as.data.table(census_final_updated_dt)

# Step 1: Calculate Theil's H
theils_h_results <- census_final_updated_dt[
  Nationality != "Majority", # Exclude the Majority group
  .(Theils_H = sum(ifelse(Proportion > 0, Proportion * log(1 / Proportion), 0), na.rm = TRUE)), # Compute Theil's H
  by = .(MSOA11CD, Year) # Group by MSOA and Year
]

# Step 2: Merge Theil's H results back into the original dataset
census_final_updated_dt <- census_final_updated_dt[
  theils_h_results,
  on = .(MSOA11CD, Year),
  Theils_H := i.Theils_H # Add Theil's H as a new column
]

```

### Isolation index

```{r}
library(data.table)

# Normalize proportions within the subset (excluding Majority)
census_final_updated_dt[
  Nationality != "Majority",
  Proportion_Subset := Proportion / sum(Proportion, na.rm = TRUE),
  by = .(MSOA11CD, Year)
]

# Calculate the Isolation Index for the subset
isolation_results_dt <- census_final_updated_dt[
  Nationality != "Majority",
  .(Isolation_Index = sum(Proportion_Subset^2, na.rm = TRUE)), # Correct calculation for the subset
  by = .(MSOA11CD, Year)
]

# Merge Isolation Index back into the original dataset
census_final_updated_dt[
  isolation_results_dt,
  on = .(MSOA11CD, Year),
  Isolation_Index := i.Isolation_Index
]

# View the updated dataset
print(census_final_updated_dt)
```

```{r}
# Final cleaning

# Filter out rows where Nationality is "Total_Population" or "Majority"
census_final_updated_dt <- census_final_updated_dt[!(Nationality %in% c("Total_Population", "Majority"))]

# Remove the columns "Total_Pop" and "Population"
census_final_updated_dt <- census_final_updated_dt[, !c("Total_Pop", "Population"), with = FALSE]
```

```{r}
# Remove rows with any NA values
census_data_clean <- na.omit(census_final_updated_dt)
```

```{r}
# Loop through each nationality and calculate statistics
for (nationality in unique(census_data_clean$Nationality)) {
  # Filter data for the current nationality
  nat_data <- census_data_clean %>%
    filter(Nationality == nationality)
  
  # Calculate statistics
  min_value <- min(nat_data$D_Index, na.rm = TRUE)
  max_value <- max(nat_data$D_Index, na.rm = TRUE)
  median_value <- median(nat_data$D_Index, na.rm = TRUE)
  mean_value <- mean(nat_data$D_Index, na.rm = TRUE)
  
  # Print results for the current nationality
  cat("Nationality:", nationality, "\n")
  cat("Min:", min_value, "\n")
  cat("Max:", max_value, "\n")
  cat("Median:", median_value, "\n")
  cat("Mean:", mean_value, "\n\n")
}
```

### Visualice histograms and boxplots of distributions

```{r}
# Directory for histogram and boxplot outputs
dir.create("distribution_plots", showWarnings = FALSE)

# Histograms for all indices
for (var in c("Proportion", "D_Index", "Isolation_Index", "Theils_H")) {
  hist_plot <- ggplot(census_data_clean, aes_string(x = var)) +
    geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
    facet_wrap(~ Nationality, scales = "free") +
    labs(title = paste("Histogram of", var, "by Nationality"),
         x = var, y = "Count") +
    theme_minimal()
  
  # Save histogram
  ggsave(filename = paste0("distribution_plots/Histogram_", var, ".png"),
         plot = hist_plot, width = 10, height = 6)
}

# Boxplots for all indices
for (var in c("Proportion", "D_Index", "Isolation_Index", "Theils_H")) {
  box_plot <- ggplot(census_data_clean, aes(x = Nationality, y = !!sym(var))) +
    geom_boxplot(fill = "lightblue", color = "black", alpha = 0.7) +
    labs(title = paste("Boxplot of", var, "by Nationality"),
         x = "Nationality", y = var) +
    theme_minimal()
  
  # Save boxplot
  ggsave(filename = paste0("distribution_plots/Boxplot_", var, ".png"),
         plot = box_plot, width = 10, height = 6)
}
```

### Visualice temporal line graphs of all indices

```{r}
library(ggplot2)
library(dplyr)

# Ensure Year is numeric
census_data_clean$Year <- as.numeric(census_data_clean$Year)

# Aggregate data by Year and Nationality
aggregated_data <- census_data_clean %>%
  group_by(Year, Nationality) %>%
  summarize(
    Proportion = mean(Proportion, na.rm = TRUE),
    D_Index = mean(D_Index, na.rm = TRUE),
    Lagged_Dissimilarity = mean(Lagged_Dissimilarity, na.rm = TRUE),
    Theils_H = mean(Theils_H, na.rm = TRUE),
    Isolation_Index = mean(Isolation_Index, na.rm = TRUE),
    .groups = "drop"
  )

# Create directory for nationality-wise plots
dir.create("line_graphs_nationality", showWarnings = FALSE)

# Loop through each nationality and create line plots
nationalities <- unique(aggregated_data$Nationality)

for (nationality in nationalities) {
  # Filter data for the current nationality
  nat_data <- aggregated_data %>%
    filter(Nationality == nationality)

  # Generate the line plot
  plot <- ggplot(nat_data) +
    geom_line(aes(x = Year, y = Proportion, color = "Proportion"), size = 1.2) +
    geom_line(aes(x = Year, y = D_Index, color = "Dissimilarity Index"), size = 1.2) +
    geom_line(aes(x = Year, y = Theils_H, color = "Theil's H"), size = 1.2) +
    geom_line(aes(x = Year, y = Isolation_Index, color = "Isolation Index"), size = 1.2) +
    labs(
      title = paste("Temporal Trends for", nationality),
      x = "Year", y = "Value", color = "Index"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      legend.position = "top"
    )

  # Save the plot
  ggsave(
    filename = paste0("line_graphs_nationality/Trends_", nationality, ".png"),
    plot = plot,
    width = 10,
    height = 6
  )
}
```

### Visualice and compare temporal line graph between all nationalities

```{r}
# Create line graph for Dissimilarity Index comparing all nationalities
dissimilarity_plot <- ggplot(aggregated_data, aes(x = Year, y = D_Index, color = Nationality, group = Nationality)) +
  geom_line(size = 1.2) +
  geom_point(size = 2) +
  scale_y_continuous(
    breaks = scales::pretty_breaks(n = 6), # Automatically generate breaks for positions
    labels = c(0.00, 0.15, 0.3, 0.45, 0.6, 0.75) # Manually specify custom labels
  ) +
  labs(
    title = "Dissimilarity Index Over Time (All Nationalities)",
    x = "Year",
    y = "Dissimilarity Index", # Add a note for clarity
    color = "Nationality" # Legend title
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title.y = element_text(face = "bold", size = 12), # Style for y-axis label
    legend.title = element_text(face = "bold"), # Bold legend title
    legend.position = "right" # Ensure the legend is visible
  )

# Save the plot
dir.create("line_graphs_comparison", showWarnings = FALSE)
ggsave(
  filename = "line_graphs_comparison/Dissimilarity_Index_All_Nationalities_Custom_Labels.png",
  plot = dissimilarity_plot, width = 10, height = 6
)
```

### Calculate spatial autocorrelation of diss index between nationalities

```{r}
library(sf)
library(dplyr)
library(ggplot2)
library(Matrix)
library(FNN)

# Ensure consistent CRS for geometries
census_data_clean <- census_data_clean %>%
  st_as_sf() %>% # Ensure it is an sf object
  st_transform(crs = 27700) # Example: British National Grid

# Create unique geometries with MSOA codes
unique_geometries <- census_data_clean %>%
  group_by(MSOA11CD) %>%
  slice(1) %>% # Retain the first row for each MSOA11CD
  ungroup() %>% # Remove grouping to avoid issues in downstream operations
  dplyr::select(MSOA11CD, geometry) %>% # Keep only MSOA11CD and geometry columns
  st_as_sf() # Ensure the result is an sf object

# Ensure CRS is consistent for unique_geometries
unique_geometries <- st_transform(unique_geometries, crs = 27700)

# Get unique nationalities
nationalities <- unique(census_data_clean$Nationality)

# Loop through each nationality
for (nationality in nationalities) {
  # Filter data for the current nationality
  nat_data <- census_data_clean %>%
    filter(Nationality == nationality) %>%
    dplyr::select(MSOA11CD, Year, D_Index, geometry) %>%
    distinct()

  # Use st_join for spatial join (ensuring proper handling of sf objects)
  nat_data <- st_join(unique_geometries, nat_data, left = TRUE)

  # Ensure nat_data is still an sf object
  nat_data <- st_as_sf(nat_data)

  # Prepare dissimilarity index values and handle missing data
  x <- nat_data$D_Index
  x[is.na(x)] <- mean(x, na.rm = TRUE)  # Replace NA with mean value
  x_mean <- mean(x, na.rm = TRUE)

  # Recompute centroids and weights matrix for this subset
  coords <- st_coordinates(st_centroid(st_geometry(nat_data)))
  knn <- get.knn(coords, k = 10)

  # Create a sparse weights matrix for this subset
  weights <- sparseMatrix(
    i = rep(1:nrow(knn$nn.index), each = 10),
    j = as.vector(knn$nn.index),
    x = 1,
    dims = c(nrow(coords), nrow(coords))
  )

  # Row-standardize the weights matrix
  row_sums <- rowSums(weights)
  row_standardized_weights <- weights / row_sums

  # Compute Local Moran's I manually
  local_moran <- numeric(length(x))
  for (i in seq_along(x)) {
    neighbors <- which(row_standardized_weights[i, ] > 0)
    lagged_value <- sum(row_standardized_weights[i, neighbors] * x[neighbors], na.rm = TRUE)
    local_moran[i] <- (x[i] - x_mean) * (lagged_value - x_mean)
  }

  # Normalize Local Moran's I
  local_moran <- local_moran / var(x, na.rm = TRUE)

  # Add Moran's I results to nat_data
  nat_data <- nat_data %>%
    mutate(Local_I = local_moran,
           Quadrant = case_when(
             (D_Index > x_mean & Local_I > 0) ~ "High-High",
             (D_Index < x_mean & Local_I > 0) ~ "Low-Low",
             (D_Index > x_mean & Local_I < 0) ~ "High-Low",
             (D_Index < x_mean & Local_I < 0) ~ "Low-High",
             TRUE ~ "Non-significant"
           ))

  # Create LISA Map
  lisa_map <- ggplot(data = nat_data) +
    geom_sf(aes(fill = Quadrant), color = NA) +
    scale_fill_manual(values = c("High-High" = "red", "Low-Low" = "blue",
                                 "High-Low" = "orange", "Low-High" = "green",
                                 "Non-significant" = "grey")) +
    labs(title = paste("LISA Map for", nationality),
         fill = "Quadrant") +
    theme_minimal()

  # Save LISA Map
  if (!dir.exists("moran_outputs")) dir.create("moran_outputs")
  ggsave(filename = paste0("moran_outputs/LISA_Map_", nationality, ".png"),
         plot = lisa_map, width = 10, height = 8)

  # Print progress
  print(paste("Completed Moran's I and LISA Map for:", nationality))
}
```

```{r}
# Moran's I	Meaning
> 0	Positive spatial autocorrelation (clustering)
< 0	Negative spatial autocorrelation (dispersion)
≈ 0	No spatial autocorrelation (random)
P-Value	Interpretation
< 0.05	Significant spatial autocorrelation
> 0.05	No significant spatial autocorrelation
```

```{r}
Mixed Red/Orange Areas

Likely represent historically segregated regions.
These regions may be where immigrant communities settled initially, creating dense cultural and ethnic clusters.
These areas may require policies aimed at fostering integration, such as:
Affordable mixed-income housing.
Educational investments to promote mobility.
Mixed Blue/Green Areas

Likely represent integrated suburban or redevelopment zones.
These areas might reflect a balance of diverse groups, driven by:
Suburban housing policies.
Urban regeneration and investment in infrastructure.
These regions might provide insights into successful policies for replication in other areas.
```

### Visualice maps of diss index between nationalities and through time

```{r}
library(ggplot2)
library(sf)

# Create directory for spatial maps
dir.create("spatial_maps", showWarnings = FALSE)

# Loop through each nationality to create maps
nationalities <- unique(census_data_clean$Nationality)

for (nationality in nationalities) {
  # Filter data for the current nationality
  nat_data <- census_data_clean %>%
    filter(Nationality == nationality)
  
  # Plot map for the current nationality
  diss_map <- ggplot(nat_data) +
    geom_sf(aes(fill = D_Index, geometry = geometry), color = NA) +
    scale_fill_viridis_c(option = "C", name = "Dissimilarity Index") +
    labs(
      title = paste("Dissimilarity Index Across London -", nationality),
      caption = "Source: Census Data"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      legend.position = "right"
    )
  
  # Save the plot
  ggsave(
    filename = paste0("spatial_maps/Dissimilarity_Index_", nationality, ".png"),
    plot = diss_map,
    width = 12,
    height = 8
  )
}
```

### Identify MSOAs with Most Changes in Dissimilarity Index

```{r}
# Calculate the top 3 boroughs (LAD) with most and least segregation
top_boroughs <- census_data_clean %>%
  group_by(LAD11NM, Nationality) %>%
  summarise(Average_Segregation = mean(D_Index, na.rm = TRUE)) %>%
  arrange(desc(Average_Segregation)) %>%
  dplyr::slice(1:3) # Top 3 most segregated

least_boroughs <- census_data_clean %>%
  group_by(LAD11NM, Nationality) %>%
  summarise(Average_Segregation = mean(D_Index, na.rm = TRUE)) %>%
  arrange(Average_Segregation) %>%
  slice(1:3) # Top 3 least segregated
```

```{r}
south america - havering, bexley, bromley
bangladesh - havering, bexley, bromley
poland - havering, barking, redbridge 
romania - bexley, bromley, sutton
caribbean - sutton, kingston, barking
se africa - sutton, kingston, barking
```

```{r}
# Calculate the top 3 boroughs with the greatest increase/decrease in segregation
segregation_change <- census_data_clean %>%
  group_by(LAD11NM, Nationality) %>%
  summarise(Change = last(D_Index) - first(D_Index)) %>%
  ungroup() %>% # Ungroup before slicing
  arrange(desc(Change)) %>%
  dplyr::slice(1:3) # Top 3 increases

segregation_decrease <- census_data_clean %>%
  group_by(LAD11NM, Nationality) %>%
  summarise(Change = last(D_Index) - first(D_Index)) %>%
  ungroup() %>% # Ungroup before slicing
  arrange(Change) %>%
  dplyr::slice(1:3) # Top 3 decreases
```

```{r}
4 msoas in croydon were the msoas with most significant increase in segregation
4 msoas in redbridge were the msoas with most significant decrease (the decrease was much more prunounced than the increase)
```

# Load all packages

```{r}
library(tidyverse)
library(sf)
library(sp)
library(spgwr)
library(ggplot2)
library(gridExtra)
library(cowplot)
library(xts)
library(zoo)
```

```{r}
# Read the datasets
setwd("/Users/anamariasteinercorrea/thesis")
housing_data <- read_csv("/Users/anamariasteinercorrea/Documents/thesis/housing_data.csv")
neighborhood_data <- read_csv("/Users/anamariasteinercorrea/Documents/thesis/neighborhood_data.csv")
msoa_geometry <- st_read("/Users/anamariasteinercorrea/Desktop/v3 longitudinal diaspora study/Data/MSOA_2011_London_gen_MHW.shp")
```

## Clean and join datasets

```{r}
# Perform the join of housing + neighborhood
hn_data <- housing_data %>%
  left_join(neighborhood_data, by = c("MSOA11NM", "MSOA11CD", "Year"))

aggregated_hn <- hn_data %>%
  group_by(MSOA11CD, MSOA11NM, Year) %>%  # Group by MSOA and Year
  summarize(across(where(is.numeric), mean, na.rm = TRUE), .groups = "drop")  # Average numeric columns
```

```{r}
segregation_analysis <- aggregated_hn %>%
  mutate(Year = as.character(Year)) %>%  # Convert Year to character in aggregated_hn
  left_join(census_data_clean %>% mutate(Year = as.character(Year)),  # Convert Year in finalest_census_dt
            by = c("MSOA11NM", "MSOA11CD", "Year"))  # Perform the join

# Check the result
print(segregation_analysis)
```

```{r}
# Join with spatial data
segregation_final <- segregation_analysis %>%
  left_join(msoa_geometry %>% distinct(MSOA11CD, MSOA11NM, .keep_all = TRUE), 
            by = c("MSOA11CD", "MSOA11NM"))

# Convert to sf object
segregation_sf <- st_as_sf(segregation_final)

# Clean the dataset
final_segregation <- segregation_sf %>%
  # Select only the necessary columns and rename them
  dplyr::select(
    MSOA11CD,
    MSOA11NM,
    Year,
    Rent,
    Socialhousing = budget_proportion,
    Owned,
    Houseprice,
    Overoccupation = Occupation,
    geometry = geometry.y,
    Crime = MSOA_crime,
    Greenery = MSOA_percent_green,
    PTAL,
    Nationality,
    Proportion,
    D_Index,
    Lagged_Dissimilarity,
  ) 

filtered_segregation <- final_segregation %>%
  filter(MSOA11NM != "London")
```

## Correlations and Linear regressions

```{r}
# Load required libraries
library(tidyverse)

# Scaling all variables
scaled_segregation <- filtered_segregation %>%
  mutate(
    # Housing variables
    scaled_Rent = -1 * (Rent / 1000),                   # Rent scale
    scaled_Socialhousing = Socialhousing / 100,        # Scale Socialhousing
    scaled_Houseprice = -1 * (Houseprice / 100000),    # Houseprice scale
    scaled_Overoccupation = Overoccupation / 1000,     # Scale Overoccupation
    scaled_Owned = Owned,                              # Keep Owned unchanged
    # Neighborhood variables
    scaled_Crime = Crime / 10,                         # Scale Crime
    scaled_Greenery = Greenery,                        # Keep Greenery unchanged
    scaled_PTAL = PTAL  
  )
```

### Correlation matrix for each nationality

```{r}
library(ggcorrplot)

# Directory for correlation matrices
dir.create("correlation_matrices", showWarnings = FALSE)

# Loop through each nationality
for (nationality in unique(scaled_segregation$Nationality)) {
  # Filter data for the current nationality and drop geometry
  nat_data <- scaled_segregation %>%
    filter(Nationality == nationality) %>%
    st_drop_geometry()  # Drop the geometry column
  
  # Subset relevant variables for correlation
  correlation_data <- nat_data %>%
    dplyr::select(
      D_Index, scaled_Rent, scaled_Socialhousing, scaled_Owned, scaled_Houseprice,
      scaled_Overoccupation, scaled_Crime, scaled_Greenery, scaled_PTAL, Lagged_Dissimilarity
    )
  
  # Remove rows with missing values
  correlation_data <- correlation_data %>%
    na.omit()
  
  # Convert all columns to numeric (if necessary)
  correlation_data <- correlation_data %>%
    mutate(across(everything(), as.numeric))
  
  # Check if enough variables remain for correlation
  if (nrow(correlation_data) > 0 && ncol(correlation_data) > 1) {
    # Compute the correlation matrix
    corr_matrix <- cor(correlation_data, use = "complete.obs")
    
    # Visualize and save the correlation matrix
    corr_plot <- ggcorrplot(corr_matrix, method = "circle",
                            title = paste("Correlation Matrix for", nationality)) +
      theme_minimal()
    
    ggsave(filename = paste0("correlation_matrices/correlation_matrix_", nationality, ".png"),
           plot = corr_plot, width = 10, height = 6)
  } else {
    message(paste("Not enough valid data for correlation for nationality:", nationality))
  }
}
```

```{r}
Variable	Direction	Strength	Description
Proportion	Positive (Red)	Strong	Higher group proportion is associated with a higher Diss_Index.
Rent	Positive (Red)	Moderate-Strong	Higher rental levels are correlated with higher Diss_Index.
Socialhousing	Positive (Red)	Moderate	Greater social housing provision correlates with higher Diss_Index.
Owned	Negative (Blue)	Moderate	Higher ownership rates are associated with lower Diss_Index.
Houseprice	Negative (Blue)	Moderate	Higher house prices correlate with lower Diss_Index (less segregation).
Overoccupation	Positive (Red)	Moderate	Higher overoccupation is associated with higher Diss_Index.
Crime	Positive (Red)	Weak-Moderate	Higher crime levels are weakly correlated with higher Diss_Index.
Greenery	Negative (Blue)	Weak	More greenery is weakly associated with lower Diss_Index.
PTAL (Transport)	Negative (Blue)	Weak	Better public transport access relates weakly to lower Diss_Index.
Lagged Variables	Mixed	Moderate	Some lagged variables (e.g., lagged_Rent, lagged_diss) show strong correlations.
3. Key Insights
Proportion:
The strongest positive correlation with Diss_Index, suggesting that areas with higher proportions of a specific group also tend to have higher segregation.
Housing Tenure:
Rent and Socialhousing are positively correlated with segregation.
Owned is negatively correlated, indicating that higher ownership rates are linked to lower levels of segregation.
Economic Factors:
Houseprice has a negative correlation, which suggests that higher house prices are linked to lower segregation, possibly due to barriers to access.
Social Indicators:
Overoccupation and Crime show positive correlations, indicating a potential association between socioeconomic stressors and higher segregation.
Environment and Accessibility:
Greenery and PTAL have weak negative correlations, meaning better access to greenery or public transport is slightly associated with lower segregation.
Lagged Variables:
Lagged versions of variables (e.g., lagged_Rent, lagged_diss) are significantly correlated, which suggests temporal persistence in relationships.
4. Implications for Modeling:
Multicollinearity Risk:
The strong correlations between Diss_Index and several predictors (e.g., Proportion, Rent, lagged_diss) suggest multicollinearity issues when building a regression or GWR model.
Use methods like VIF or regularization (Ridge/Lasso) to manage this.
Variable Selection:
Consider dropping or transforming highly correlated predictors (e.g., between Proportion and Diss_Index).
Alternatively, Proportion could serve as a competing dependent variable.
Potential Predictors:
Variables such as Owned, Houseprice, Overoccupation, and Socialhousing can provide meaningful relationships with Diss_Index, as they show moderate correlations and distinct associations.
```

```{r}
# Load required libraries
library(dplyr)

predictors <- scaled_segregation %>%
  st_drop_geometry() %>%  
  dplyr::select(scaled_Rent, scaled_Socialhousing, scaled_Owned, scaled_Houseprice,
      scaled_Overoccupation, scaled_Crime, scaled_Greenery, scaled_PTAL,
      Lagged_Dissimilarity) %>% 
  na.omit()  # Ensure no missing values

# Function to calculate VIF manually
calculate_vif <- function(data) {
  vif_values <- sapply(names(data), function(var) {
    # Create a formula regressing each variable on all others
    formula <- as.formula(paste(var, "~ ."))
    r_squared <- summary(lm(formula, data = data))$r.squared  # Get R²
    vif <- 1 / (1 - r_squared)  # Calculate VIF
    return(vif)
  })
  return(vif_values)
}

# Calculate VIF for predictors
vif_results <- calculate_vif(predictors)

# Print VIF results
print(vif_results)
```

```{r}
The latest VIF results indicate that all predictors fall within acceptable thresholds, with values ranging from 1.29 to 3.97, suggesting minimal multicollinearity. Variables such as Proportion (1.33), Socialhousing (1.29), Rent (1.54), and Houseprice (1.87) show no multicollinearity concerns, while Overoccupation (3.97) and lagged_diss (3.11) remain slightly higher but still well below problematic levels. These results confirm that the predictors can be included in the model without destabilizing the regression coefficients. Moving forward, I will continue to monitor VIF values when introducing new variables or adjustments to ensure multicollinearity remains within acceptable bounds, maintaining the robustness and interpretability of the analysis.
```

### Scatterplot for individual correlations

```{r}
library(ggplot2)
library(dplyr)
library(MASS)  # For robust regression

# Ensure directories for outputs exist
if (!dir.exists("scatterplots_fixed")) dir.create("scatterplots_fixed")
if (!dir.exists("regression_outputs")) dir.create("regression_outputs")

# Specify variables for scatterplots and regression
y_vars <- c("scaled_Rent", "scaled_Socialhousing", "scaled_Owned", "scaled_Houseprice",
            "scaled_Overoccupation", "scaled_Crime", "scaled_Greenery", "scaled_PTAL", "Lagged_Dissimilarity")

# Loop through each nationality
for (nationality in unique(scaled_segregation$Nationality)) {
  # Filter data for the current nationality
  nat_data <- scaled_segregation %>%
    filter(Nationality == nationality) %>%
    mutate(Dissimilarity_Index = as.numeric(D_Index))  # Ensure numeric
  
  # Create PDF for diagnostics
  pdf(file = paste0("regression_outputs/Diagnostics_", nationality, ".pdf"))
  
  # Loop through each variable
  for (y_var in y_vars) {
    if (y_var %in% colnames(nat_data)) {  # Ensure variable exists
      # Clean the data: remove NA and outliers
      clean_data <- nat_data %>%
        filter(
          !is.na(Dissimilarity_Index), !is.na(!!sym(y_var)),
          Dissimilarity_Index < quantile(D_Index, 0.99, na.rm = TRUE),
          !!sym(y_var) < quantile(!!sym(y_var), 0.99, na.rm = TRUE)
        )
      
      # Skip if not enough data
      if (nrow(clean_data) < 5) {
        warning(paste("Not enough data for regression for nationality:", nationality, "and predictor:", y_var))
        next
      }
      
      # Fit simple regression model (lm by default)
      regression_formula <- paste("D_Index ~", y_var)
      model <- lm(as.formula(regression_formula), data = clean_data)
      
      # Check diagnostics: Heteroscedasticity
      diagnostics_problem <- function(model) {
        res <- residuals(model)
        fitted_vals <- fitted(model)
        abs(cor(res^2, fitted_vals)) > 0.3  # Example threshold
      }
      
      # Switch to robust regression if diagnostics indicate issues
      if (diagnostics_problem(model)) {
        message(paste("Switching to robust regression for:", nationality, "-", y_var))
        model <- rlm(as.formula(regression_formula), data = clean_data)
      }
      
      # Create diagnostics plots
      par(mfrow = c(2, 2))  # 2x2 layout for diagnostics
      plot(model, main = paste("Diagnostics for", nationality, "-", y_var))
      
      # Create scatterplot
      scatter_plot <- ggplot(clean_data, aes(x = D_Index, y = !!sym(y_var))) +
        geom_point(alpha = 0.5, color = "blue") +
        geom_smooth(method = "lm", se = FALSE, color = "red") +
        labs(
          title = paste("Scatterplot for", nationality, "-", y_var),
          x = "Dissimilarity Index",
          y = y_var
        ) +
        theme_minimal() +
        scale_y_continuous(expand = c(0.2, 0))
      
      # Save scatterplot
      ggsave(
        filename = paste0("scatterplots_fixed/scatterplot_", nationality, "_", y_var, ".png"),
        plot = scatter_plot,
        width = 10,
        height = 6
      )
    }
  }
  
  dev.off()  # Close PDF for diagnostics
}
```

```{r}
ggplot(scaled_segregation, aes(x = scaled_Rent, y = D_Index, color = factor(cut(scaled_Houseprice, breaks = 3)))) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(color = "House Price Level")

```


### CCFs for individual correlations

```{r}
library(dplyr)

# Ensure a directory for CCF outputs
if (!dir.exists("ccfs")) dir.create("ccfs")

# List of variables to analyze against Dissimilarity_Index
x_vars <- c("scaled_Rent", "scaled_Socialhousing", "scaled_Houseprice", "scaled_Overoccupation", "scaled_Owned")

# Aggregate data over all MSOAs by year and nationality
aggregated_data <- scaled_segregation %>%
  group_by(Year, Nationality) %>%
  summarize(
    mean_diss = mean(D_Index, na.rm = TRUE),  # Average Dissimilarity Index
    across(all_of(x_vars), ~ mean(.x, na.rm = TRUE)),  # Aggregate housing variables
    .groups = "drop"
  ) %>%
  na.omit()  # Remove rows with NA values after aggregation

# Check if aggregation was successful
if (nrow(aggregated_data) == 0) {
  stop("No valid data after aggregation. Check input dataset.")
}

# Loop through each nationality
for (nationality in unique(aggregated_data$Nationality)) {
  # Filter data for the current nationality
  nat_data <- aggregated_data %>%
    filter(Nationality == nationality) %>%
    arrange(Year)  # Ensure data is sorted by year

  # Check if sufficient data exists
  if (nrow(nat_data) < 5) {  # Require at least 5 years of data for meaningful CCF
    message(paste("Skipping Nationality:", nationality, "- Not enough data"))
    next
  }

  # Loop through each variable to compute CCF
  for (x_var in x_vars) {
    if (x_var %in% colnames(nat_data)) {  # Ensure variable exists
      # Extract data for CCF
      y_data <- nat_data$mean_diss
      x_data <- nat_data[[x_var]]

      # Check if sufficient and valid data exists
      if (var(y_data) > 0 && var(x_data) > 0) {  # Avoid constant or invalid data
        # Save CCF plot
        png(filename = paste0("ccfs/ccf_London_", nationality, "_", x_var, ".png"), width = 800, height = 600)
        ccf(
          y_data, x_data,
          lag.max = 10, plot = TRUE,
          main = paste("CCF for London - Nationality:", nationality, "-", x_var)
        )
        dev.off()
      } else {
        message(paste("Skipping Nationality:", nationality, "Variable:", x_var, "- Insufficient or constant data"))
      }
    } else {
      message(paste("Variable", x_var, "not found for Nationality:", nationality))
    }
  }
}
```

### Linear regressions

Base regressions (1-5 housing characteristics) Run separate regressions for each housing variable to assess their individual contributions to the segregation index. No. 6 Neighborhood data Add neighborhood-level variables to create a comprehensive model. No. 7 Lagged settlement model Incorporate the lagged settlement variable alongside all housing variables to capture temporal dependencies.

```{r}
# Load necessary libraries
library(stargazer)  # For LaTeX/HTML-style regression tables
library(dplyr)      # For data manipulation

# Define variable groups
housing_variables <- c("scaled_Rent", "scaled_Socialhousing", "scaled_Houseprice", "scaled_Overoccupation", "scaled_Owned")
neighborhood_variables <- c("scaled_Crime", "scaled_Greenery", "scaled_PTAL")
all_variables <- c(housing_variables, "Lagged_Dissimilarity", neighborhood_variables)

# Create directory for outputs
output_dir <- "regression_outputs"
if (!dir.exists(output_dir)) dir.create(output_dir)

# Loop through each nationality
for (nationality in unique(scaled_segregation$Nationality)) {
  # Filter data for the current nationality
  nationality_data <- scaled_segregation %>%
    filter(Nationality == nationality)
  
  # Skip if no data is available
  if (nrow(nationality_data) == 0) {
    warning(paste("No valid data for nationality:", nationality))
    next
  }
  
  # Fit original progressive models
  models <- list(
    lm(D_Index ~ scaled_Rent + scaled_Socialhousing + scaled_Houseprice, data = nationality_data),
    lm(D_Index ~ scaled_Rent + scaled_Socialhousing + scaled_Houseprice + scaled_Overoccupation, data = nationality_data),
    lm(D_Index ~ scaled_Rent + scaled_Socialhousing + scaled_Houseprice + scaled_Overoccupation + scaled_Owned, data = nationality_data),
    lm(D_Index ~ scaled_Rent + scaled_Socialhousing + scaled_Houseprice + scaled_Overoccupation + scaled_Owned + Lagged_Dissimilarity, data = nationality_data),
    lm(D_Index ~ scaled_Rent + scaled_Socialhousing + scaled_Houseprice + scaled_Overoccupation + scaled_Owned + Lagged_Dissimilarity +
         scaled_Crime + scaled_Greenery + scaled_PTAL, data = nationality_data),
    # Include Rent × House Price interaction
    lm(D_Index ~ scaled_Rent * scaled_Houseprice + scaled_Socialhousing + scaled_Overoccupation + scaled_Owned + Lagged_Dissimilarity +
         scaled_Crime + scaled_Greenery + scaled_PTAL, data = nationality_data)
  )
  
  # Generate HTML regression table
  stargazer(
    models,
    type = "html",
    out = paste0(output_dir, "/RegressionTable_", nationality, ".html"),
    title = paste("Regression Results for", nationality, "(Including Rent × House Price Interaction)"),
    dep.var.labels = "Dissimilarity Index",
    covariate.labels = c(
      "Rent", "Social Housing", "House Price", "Overoccupation", "Owned", "Lagged Dissimilarity",
      "Crime", "Greenery", "PTAL", "Rent × House Price"
    ),
    star.cutoffs = c(0.05, 0.01, 0.001),
    digits = 3,
    coef = lapply(all_models, function(model) { coef(model) * 10 }),  # Scale coefficients
    se = lapply(all_models, function(model) { summary(model)$coefficients[, "Std. Error"] * 10 })  # Scale standard errors
  )
}
```

## GWR

```{r}
library(dplyr)
library(sf)

# Ensure Nationality is preserved in the aggregation step
aggregated_data <- scaled_segregation %>%
  group_by(MSOA11CD, MSOA11NM, geometry, Nationality) %>%  # Include Nationality
  summarise(
    scaled_Rent = mean(scaled_Rent, na.rm = TRUE),
    scaled_Socialhousing = mean(scaled_Socialhousing, na.rm = TRUE),
    scaled_Houseprice = mean(scaled_Houseprice, na.rm = TRUE),
    scaled_Overoccupation = mean(scaled_Overoccupation, na.rm = TRUE),
    scaled_Owned = mean(scaled_Owned, na.rm = TRUE),
    Lagged_Dissimilarity = mean(Lagged_Dissimilarity, na.rm = TRUE),
    scaled_Crime = mean(scaled_Crime, na.rm = TRUE),
    scaled_Greenery = mean(scaled_Greenery, na.rm = TRUE),
    scaled_PTAL = mean(scaled_PTAL, na.rm = TRUE),
    D_Index = mean(D_Index, na.rm = TRUE)
  ) %>%
  ungroup()

# Ensure the result is an sf object
aggregated_data <- st_as_sf(aggregated_data)
```

```{r}
# Install spatial regression libraries if needed
library(sf)         # For handling spatial data
library(spgwr)  
```

```{r}
# Remove NaN rows from numeric fields
aggregated_data <- aggregated_data %>%
  filter(across(where(is.numeric), ~ !is.nan(.)))

# Ensure Nationality is not missing
aggregated_data <- aggregated_data %>%
  filter(!is.na(Nationality))
```

```{r}
# Ensure valid geometries and compute centroids
aggregated_data <- aggregated_data %>%
  st_make_valid() %>%
  mutate(centroid = st_centroid(geometry))
```

```{r}
library(sf)
library(spgwr)

# Ensure valid geometries and reproject to a projected CRS
aggregated_data <- st_make_valid(aggregated_data)
aggregated_data <- st_transform(aggregated_data, crs = 32630)

# Get list of unique nationalities
nationalities <- unique(aggregated_data$Nationality)

# Initialize list to store results
gwr_results_by_nationality <- list()

# Loop through each nationality
for (nat in nationalities) {
  message(paste("Processing nationality:", nat))
  
  # Filter data for the specific nationality
  data_subset <- aggregated_data %>% filter(Nationality == nat)
  
  # Ensure there are enough data points
  if (nrow(data_subset) < 10) {
    message(paste("Skipping nationality:", nat, "- insufficient data"))
    next
  }
  
  # Extract coordinates from centroids
  coords <- st_coordinates(data_subset$centroid)
  print(paste("Coordinates for", nat, ":", head(coords)))
  
  # Check coords validity
  if (!is.matrix(coords) || ncol(coords) != 2 || !is.numeric(coords)) {
    message(paste("Invalid coordinates for nationality:", nat))
    next
  }
  
  # Define GWR formula
  gwr_formula <- D_Index ~ scaled_Rent + scaled_Socialhousing + scaled_Houseprice + 
                 scaled_Overoccupation + scaled_Owned + Lagged_Dissimilarity + 
                 scaled_Crime + scaled_Greenery + scaled_PTAL
  
  # Select bandwidth or use default
  bandwidth <- tryCatch({
    gwr.sel(
      formula = gwr_formula,
      data = data_subset,
      coords = coords,
      adapt = TRUE
    )
  }, error = function(e) {
    message(paste("Bandwidth selection failed for nationality:", nat, "Error:", e$message))
    NA
  })
  
  # Debugging bandwidth
  print(paste("Bandwidth for", nat, ":", bandwidth))
  
  # Check bandwidth validity
  if (is.na(bandwidth) || !is.numeric(bandwidth)) {
    message(paste("Invalid bandwidth for nationality:", nat, "- using default value"))
    bandwidth <- 0.1  # Default adaptive bandwidth
  }
  
  # Run GWR
  gwr_result <- tryCatch({
    gwr(
      formula = gwr_formula,
      data = data_subset,
      coords = coords,
      bandwidth = bandwidth,
      adapt = TRUE,
      fit.points = coords
    )
  }, error = function(e) {
    message(paste("GWR failed for nationality:", nat, "Error:", e$message))
    NULL
  })
  
  # Store results if valid
  if (!is.null(gwr_result)) {
    gwr_results_by_nationality[[nat]] <- gwr_result
    message(paste("Completed GWR for nationality:", nat))
  }
}

# Print summary of GWR results for each nationality
for (nat in names(gwr_results_by_nationality)) {
  if (!is.null(gwr_results_by_nationality[[nat]])) {
    cat(paste("\nResults for Nationality:", nat, "\n"))
    print(gwr_results_by_nationality[[nat]])
  }
}
```

```{r}
library(ggplot2)
library(sf)

# Example: Visualize Rent coefficients for a specific nationality
nat_to_visualize <- "Bangladesh"
data_to_visualize <- gwr_results_by_nationality[[nat_to_visualize]]$SDF

# Convert to sf object
data_to_visualize_sf <- st_as_sf(data_to_visualize, coords = c("X", "Y"), crs = 32630)

# Plot using ggplot2
ggplot(data_to_visualize_sf) +
  geom_sf(aes(fill = scaled_Rent), color = NA) +
  scale_fill_viridis_c(option = "C", name = "Rent Coefficient") +
  labs(
    title = paste("GWR Coefficients for Rent -", nat_to_visualize),
    subtitle = "Geographical variation in Rent coefficient",
    caption = "Source: GWR Analysis"
  ) +
  theme_minimal()
```
